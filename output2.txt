Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: 8016efff07cc004d65c1e9eff314f38792840c97, status: has uncommited changes, branch: QCRI_Cifar10_same_batch_augmentation

arch: vit_tiny
batch_size_per_gpu: 40
clip_grad: 3.0
data_path: /home/alij/Datasets/Cifar10/train
dist_url: env://
drop_path_rate: 0.1
epochs: 103
freeze_last_layer: 1
global_crops_number: 2
global_crops_scale: (0.4, 1.0)
global_scale: 224
gpu: 0
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
local_scale: 96
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
norm_last_layer: True
num_workers: 4
optimizer: adamw
out_dim: 1000
output_dir: /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu
patch_size: 16
rank: 0
saveckp_freq: 20
seed: 0
teacher_temp: 0.04
use_bn_in_head: False
use_fp16: True
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 50000 images.
Student and Teacher are built: they are both vit_tiny network.
Loss, optimizer and schedulers ready.
Found checkpoint at /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth
=> loaded 'student' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'fp16_scaler' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
Starting DINO training !
Epoch: [102/103]  [   0/1250]  eta: 1:50:14  loss: 2.913065 (2.913065)  lr: 0.000001 (0.000001)  wd: 0.399916 (0.399916)  time: 5.291256  data: 2.273594  max mem: 5312
Epoch: [102/103]  [  10/1250]  eta: 0:14:48  loss: 2.400138 (2.480333)  lr: 0.000001 (0.000001)  wd: 0.399917 (0.399917)  time: 0.716168  data: 0.226222  max mem: 5407
Epoch: [102/103]  [  20/1250]  eta: 0:11:40  loss: 2.400138 (2.595365)  lr: 0.000001 (0.000001)  wd: 0.399918 (0.399918)  time: 0.333414  data: 0.097460  max mem: 5409
Epoch: [102/103]  [  30/1250]  eta: 0:10:47  loss: 2.645875 (2.579503)  lr: 0.000001 (0.000001)  wd: 0.399919 (0.399918)  time: 0.429182  data: 0.183444  max mem: 5414
Epoch: [102/103]  [  40/1250]  eta: 0:10:20  loss: 2.612048 (2.589647)  lr: 0.000001 (0.000001)  wd: 0.399920 (0.399919)  time: 0.453036  data: 0.203503  max mem: 5414
Epoch: [102/103]  [  50/1250]  eta: 0:09:49  loss: 2.442663 (2.562335)  lr: 0.000001 (0.000001)  wd: 0.399922 (0.399920)  time: 0.429244  data: 0.193168  max mem: 5414
Epoch: [102/103]  [  60/1250]  eta: 0:09:46  loss: 2.400840 (2.553458)  lr: 0.000001 (0.000001)  wd: 0.399923 (0.399920)  time: 0.452057  data: 0.215292  max mem: 5430
Epoch: [102/103]  [  70/1250]  eta: 0:09:28  loss: 2.652979 (2.591832)  lr: 0.000001 (0.000001)  wd: 0.399924 (0.399921)  time: 0.456905  data: 0.214326  max mem: 5430
Epoch: [102/103]  [  80/1250]  eta: 0:09:25  loss: 2.675290 (2.593875)  lr: 0.000001 (0.000001)  wd: 0.399925 (0.399922)  time: 0.455513  data: 0.217962  max mem: 5430
Epoch: [102/103]  [  90/1250]  eta: 0:09:09  loss: 2.675290 (2.603555)  lr: 0.000001 (0.000001)  wd: 0.399927 (0.399922)  time: 0.444570  data: 0.206525  max mem: 5430
Epoch: [102/103]  [ 100/1250]  eta: 0:08:59  loss: 2.685608 (2.615601)  lr: 0.000001 (0.000001)  wd: 0.399928 (0.399923)  time: 0.411803  data: 0.172802  max mem: 5430
Epoch: [102/103]  [ 110/1250]  eta: 0:08:47  loss: 2.570138 (2.608066)  lr: 0.000001 (0.000001)  wd: 0.399929 (0.399923)  time: 0.413726  data: 0.179013  max mem: 5430
Epoch: [102/103]  [ 120/1250]  eta: 0:08:42  loss: 2.512784 (2.598365)  lr: 0.000001 (0.000001)  wd: 0.399930 (0.399924)  time: 0.428520  data: 0.184108  max mem: 5430
Epoch: [102/103]  [ 130/1250]  eta: 0:08:33  loss: 2.512784 (2.588495)  lr: 0.000001 (0.000001)  wd: 0.399932 (0.399925)  time: 0.436915  data: 0.193449  max mem: 5430
Epoch: [102/103]  [ 140/1250]  eta: 0:08:27  loss: 2.546027 (2.583232)  lr: 0.000001 (0.000001)  wd: 0.399933 (0.399925)  time: 0.424401  data: 0.186811  max mem: 5430
Epoch: [102/103]  [ 150/1250]  eta: 0:08:19  loss: 2.433687 (2.576854)  lr: 0.000001 (0.000001)  wd: 0.399934 (0.399926)  time: 0.424993  data: 0.186552  max mem: 5430
Epoch: [102/103]  [ 160/1250]  eta: 0:08:18  loss: 2.561565 (2.593997)  lr: 0.000001 (0.000001)  wd: 0.399935 (0.399927)  time: 0.455697  data: 0.219629  max mem: 5430
Epoch: [102/103]  [ 170/1250]  eta: 0:08:09  loss: 2.591014 (2.583375)  lr: 0.000001 (0.000001)  wd: 0.399936 (0.399927)  time: 0.441823  data: 0.204554  max mem: 5430
Epoch: [102/103]  [ 180/1250]  eta: 0:08:04  loss: 2.457074 (2.580652)  lr: 0.000001 (0.000001)  wd: 0.399938 (0.399928)  time: 0.418331  data: 0.175570  max mem: 5430
Epoch: [102/103]  [ 190/1250]  eta: 0:07:58  loss: 2.552194 (2.584816)  lr: 0.000001 (0.000001)  wd: 0.399939 (0.399928)  time: 0.437126  data: 0.193441  max mem: 5430
Epoch: [102/103]  [ 200/1250]  eta: 0:07:57  loss: 2.557700 (2.588129)  lr: 0.000001 (0.000001)  wd: 0.399940 (0.399929)  time: 0.470926  data: 0.235306  max mem: 5430
Epoch: [102/103]  [ 210/1250]  eta: 0:07:50  loss: 2.641612 (2.596695)  lr: 0.000001 (0.000001)  wd: 0.399941 (0.399930)  time: 0.463131  data: 0.227787  max mem: 5430
Epoch: [102/103]  [ 220/1250]  eta: 0:07:43  loss: 2.856753 (2.609940)  lr: 0.000001 (0.000001)  wd: 0.399942 (0.399930)  time: 0.408054  data: 0.172309  max mem: 5430
Epoch: [102/103]  [ 230/1250]  eta: 0:07:37  loss: 2.492660 (2.609083)  lr: 0.000001 (0.000001)  wd: 0.399943 (0.399931)  time: 0.406958  data: 0.172679  max mem: 5430
Epoch: [102/103]  [ 240/1250]  eta: 0:07:34  loss: 2.665170 (2.616764)  lr: 0.000001 (0.000001)  wd: 0.399944 (0.399931)  time: 0.450303  data: 0.214047  max mem: 5430
Epoch: [102/103]  [ 250/1250]  eta: 0:07:29  loss: 2.785872 (2.624082)  lr: 0.000001 (0.000001)  wd: 0.399945 (0.399932)  time: 0.459419  data: 0.221078  max mem: 5430
Epoch: [102/103]  [ 260/1250]  eta: 0:07:25  loss: 2.594012 (2.621705)  lr: 0.000001 (0.000001)  wd: 0.399946 (0.399932)  time: 0.448921  data: 0.208092  max mem: 5430
Epoch: [102/103]  [ 270/1250]  eta: 0:07:19  loss: 2.577138 (2.622950)  lr: 0.000001 (0.000001)  wd: 0.399947 (0.399933)  time: 0.440426  data: 0.196713  max mem: 5430
Epoch: [102/103]  [ 280/1250]  eta: 0:07:14  loss: 2.443511 (2.616244)  lr: 0.000001 (0.000001)  wd: 0.399949 (0.399934)  time: 0.421252  data: 0.175198  max mem: 5430
Epoch: [102/103]  [ 290/1250]  eta: 0:07:08  loss: 2.396876 (2.608045)  lr: 0.000001 (0.000001)  wd: 0.399950 (0.399934)  time: 0.413582  data: 0.171198  max mem: 5430
Epoch: [102/103]  [ 300/1250]  eta: 0:07:05  loss: 2.414701 (2.607747)  lr: 0.000001 (0.000001)  wd: 0.399951 (0.399935)  time: 0.439275  data: 0.200763  max mem: 5430
Epoch: [102/103]  [ 310/1250]  eta: 0:06:59  loss: 2.494560 (2.608700)  lr: 0.000001 (0.000001)  wd: 0.399952 (0.399935)  time: 0.447363  data: 0.200791  max mem: 5430
Epoch: [102/103]  [ 320/1250]  eta: 0:06:56  loss: 2.649983 (2.615536)  lr: 0.000001 (0.000001)  wd: 0.399953 (0.399936)  time: 0.445546  data: 0.190184  max mem: 5430
Epoch: [102/103]  [ 330/1250]  eta: 0:06:51  loss: 2.670399 (2.616471)  lr: 0.000001 (0.000001)  wd: 0.399954 (0.399936)  time: 0.456511  data: 0.202674  max mem: 5430
Epoch: [102/103]  [ 340/1250]  eta: 0:06:46  loss: 2.631404 (2.617125)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399937)  time: 0.443667  data: 0.194077  max mem: 5430
Epoch: [102/103]  [ 350/1250]  eta: 0:06:41  loss: 2.579864 (2.612843)  lr: 0.000001 (0.000001)  wd: 0.399956 (0.399938)  time: 0.431232  data: 0.187851  max mem: 5430
Epoch: [102/103]  [ 360/1250]  eta: 0:06:37  loss: 2.536072 (2.612149)  lr: 0.000001 (0.000001)  wd: 0.399957 (0.399938)  time: 0.444020  data: 0.200978  max mem: 5430
Epoch: [102/103]  [ 370/1250]  eta: 0:06:31  loss: 2.549536 (2.608798)  lr: 0.000001 (0.000001)  wd: 0.399958 (0.399939)  time: 0.433270  data: 0.188395  max mem: 5430
Epoch: [102/103]  [ 380/1250]  eta: 0:06:28  loss: 2.549536 (2.608080)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399939)  time: 0.427220  data: 0.184191  max mem: 5430
Epoch: [102/103]  [ 390/1250]  eta: 0:06:22  loss: 2.576097 (2.604776)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399940)  time: 0.444527  data: 0.203198  max mem: 5431
Epoch: [102/103]  [ 400/1250]  eta: 0:06:18  loss: 2.478946 (2.602867)  lr: 0.000001 (0.000001)  wd: 0.399960 (0.399940)  time: 0.439109  data: 0.201799  max mem: 5431
Epoch: [102/103]  [ 410/1250]  eta: 0:06:13  loss: 2.528740 (2.603554)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399941)  time: 0.438317  data: 0.196601  max mem: 5431
Epoch: [102/103]  [ 420/1250]  eta: 0:06:09  loss: 2.615176 (2.604914)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399941)  time: 0.426310  data: 0.183136  max mem: 5431
Epoch: [102/103]  [ 430/1250]  eta: 0:06:03  loss: 2.542112 (2.604185)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399942)  time: 0.405110  data: 0.162919  max mem: 5431
Epoch: [102/103]  [ 440/1250]  eta: 0:05:58  loss: 2.509440 (2.605118)  lr: 0.000001 (0.000001)  wd: 0.399964 (0.399942)  time: 0.397925  data: 0.159539  max mem: 5431
Epoch: [102/103]  [ 450/1250]  eta: 0:05:52  loss: 2.553361 (2.603232)  lr: 0.000001 (0.000001)  wd: 0.399965 (0.399943)  time: 0.401239  data: 0.163439  max mem: 5431
Epoch: [102/103]  [ 460/1250]  eta: 0:05:49  loss: 2.422390 (2.603737)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399943)  time: 0.433416  data: 0.190644  max mem: 5431
Epoch: [102/103]  [ 470/1250]  eta: 0:05:44  loss: 2.558069 (2.605704)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399944)  time: 0.449964  data: 0.212652  max mem: 5431
Epoch: [102/103]  [ 480/1250]  eta: 0:05:41  loss: 2.524385 (2.603724)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399944)  time: 0.461848  data: 0.225080  max mem: 5431
Epoch: [102/103]  [ 490/1250]  eta: 0:05:36  loss: 2.443443 (2.603633)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399945)  time: 0.455942  data: 0.216873  max mem: 5431
Epoch: [102/103]  [ 500/1250]  eta: 0:05:31  loss: 2.418638 (2.602128)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399945)  time: 0.427295  data: 0.188082  max mem: 5431
Epoch: [102/103]  [ 510/1250]  eta: 0:05:26  loss: 2.544498 (2.601633)  lr: 0.000001 (0.000001)  wd: 0.399970 (0.399946)  time: 0.434890  data: 0.195156  max mem: 5431
Epoch: [102/103]  [ 520/1250]  eta: 0:05:22  loss: 2.622619 (2.601489)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399946)  time: 0.420842  data: 0.183413  max mem: 5431
Epoch: [102/103]  [ 530/1250]  eta: 0:05:17  loss: 2.646751 (2.602233)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399947)  time: 0.405916  data: 0.170781  max mem: 5431
Epoch: [102/103]  [ 540/1250]  eta: 0:05:12  loss: 2.497825 (2.601725)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399947)  time: 0.411150  data: 0.174655  max mem: 5431
Epoch: [102/103]  [ 550/1250]  eta: 0:05:07  loss: 2.497825 (2.604099)  lr: 0.000001 (0.000001)  wd: 0.399973 (0.399948)  time: 0.407756  data: 0.166202  max mem: 5431
Epoch: [102/103]  [ 560/1250]  eta: 0:05:03  loss: 2.493447 (2.600517)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399948)  time: 0.430117  data: 0.191183  max mem: 5431
Epoch: [102/103]  [ 570/1250]  eta: 0:04:58  loss: 2.349038 (2.598132)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399949)  time: 0.443424  data: 0.204728  max mem: 5431
Epoch: [102/103]  [ 580/1250]  eta: 0:04:54  loss: 2.445229 (2.598411)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399949)  time: 0.441934  data: 0.195671  max mem: 5431
Epoch: [102/103]  [ 590/1250]  eta: 0:04:50  loss: 2.573241 (2.601168)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399950)  time: 0.441219  data: 0.199801  max mem: 5431
Epoch: [102/103]  [ 600/1250]  eta: 0:04:45  loss: 2.573241 (2.599463)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399950)  time: 0.432225  data: 0.191604  max mem: 5431
Epoch: [102/103]  [ 610/1250]  eta: 0:04:41  loss: 2.561270 (2.600911)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399950)  time: 0.442551  data: 0.194836  max mem: 5431
Epoch: [102/103]  [ 620/1250]  eta: 0:04:37  loss: 2.581809 (2.604169)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399951)  time: 0.440560  data: 0.197948  max mem: 5431
Epoch: [102/103]  [ 630/1250]  eta: 0:04:32  loss: 2.595697 (2.606021)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399951)  time: 0.411296  data: 0.173167  max mem: 5431
Epoch: [102/103]  [ 640/1250]  eta: 0:04:27  loss: 2.644606 (2.607754)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399952)  time: 0.418441  data: 0.176080  max mem: 5431
Epoch: [102/103]  [ 650/1250]  eta: 0:04:22  loss: 2.531823 (2.605854)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399952)  time: 0.419841  data: 0.176976  max mem: 5431
Epoch: [102/103]  [ 660/1250]  eta: 0:04:18  loss: 2.531823 (2.608156)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399953)  time: 0.405847  data: 0.159987  max mem: 5431
Epoch: [102/103]  [ 670/1250]  eta: 0:04:13  loss: 2.626026 (2.606895)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399953)  time: 0.422184  data: 0.174763  max mem: 5431
Epoch: [102/103]  [ 680/1250]  eta: 0:04:09  loss: 2.450733 (2.603932)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399954)  time: 0.428202  data: 0.186377  max mem: 5431
Epoch: [102/103]  [ 690/1250]  eta: 0:04:05  loss: 2.453228 (2.603439)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399954)  time: 0.439449  data: 0.200056  max mem: 5431
Epoch: [102/103]  [ 700/1250]  eta: 0:04:01  loss: 2.436977 (2.601827)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399954)  time: 0.446748  data: 0.205678  max mem: 5431
Epoch: [102/103]  [ 710/1250]  eta: 0:03:56  loss: 2.567476 (2.602914)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399955)  time: 0.450794  data: 0.212428  max mem: 5431
Epoch: [102/103]  [ 720/1250]  eta: 0:03:52  loss: 2.587019 (2.602338)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399955)  time: 0.458961  data: 0.222661  max mem: 5431
Epoch: [102/103]  [ 730/1250]  eta: 0:03:47  loss: 2.513524 (2.601599)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399956)  time: 0.437616  data: 0.193114  max mem: 5431
Epoch: [102/103]  [ 740/1250]  eta: 0:03:43  loss: 2.603820 (2.604799)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399956)  time: 0.414440  data: 0.175748  max mem: 5431
Epoch: [102/103]  [ 750/1250]  eta: 0:03:38  loss: 2.651782 (2.604432)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399956)  time: 0.430462  data: 0.196592  max mem: 5431
Epoch: [102/103]  [ 760/1250]  eta: 0:03:35  loss: 2.651782 (2.609137)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399957)  time: 0.465968  data: 0.222764  max mem: 5431
Epoch: [102/103]  [ 770/1250]  eta: 0:03:30  loss: 2.645244 (2.609101)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399957)  time: 0.448369  data: 0.201293  max mem: 5431
Epoch: [102/103]  [ 780/1250]  eta: 0:03:26  loss: 2.541318 (2.608795)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399958)  time: 0.427143  data: 0.180075  max mem: 5431
Epoch: [102/103]  [ 790/1250]  eta: 0:03:21  loss: 2.541318 (2.607223)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399958)  time: 0.446973  data: 0.204242  max mem: 5431
Epoch: [102/103]  [ 800/1250]  eta: 0:03:17  loss: 2.442970 (2.604666)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399958)  time: 0.430035  data: 0.189866  max mem: 5431
Epoch: [102/103]  [ 810/1250]  eta: 0:03:12  loss: 2.431811 (2.604614)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399959)  time: 0.400889  data: 0.161482  max mem: 5431
Epoch: [102/103]  [ 820/1250]  eta: 0:03:08  loss: 2.468611 (2.603259)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399959)  time: 0.405992  data: 0.169161  max mem: 5431
Epoch: [102/103]  [ 830/1250]  eta: 0:03:03  loss: 2.619558 (2.606833)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399960)  time: 0.428619  data: 0.184042  max mem: 5431
Epoch: [102/103]  [ 840/1250]  eta: 0:02:59  loss: 2.723911 (2.606173)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399960)  time: 0.435862  data: 0.185078  max mem: 5431
Epoch: [102/103]  [ 850/1250]  eta: 0:02:54  loss: 2.512053 (2.605585)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399960)  time: 0.418923  data: 0.176911  max mem: 5431
Epoch: [102/103]  [ 860/1250]  eta: 0:02:50  loss: 2.512053 (2.604495)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399961)  time: 0.421032  data: 0.185542  max mem: 5431
Epoch: [102/103]  [ 870/1250]  eta: 0:02:45  loss: 2.571453 (2.604357)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399961)  time: 0.429380  data: 0.193355  max mem: 5431
Epoch: [102/103]  [ 880/1250]  eta: 0:02:41  loss: 2.691858 (2.606923)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399961)  time: 0.444553  data: 0.199429  max mem: 5431
Epoch: [102/103]  [ 890/1250]  eta: 0:02:37  loss: 2.679880 (2.606509)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399962)  time: 0.431765  data: 0.183690  max mem: 5431
Epoch: [102/103]  [ 900/1250]  eta: 0:02:32  loss: 2.596854 (2.606134)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399962)  time: 0.420525  data: 0.180030  max mem: 5431
Epoch: [102/103]  [ 910/1250]  eta: 0:02:28  loss: 2.582237 (2.606429)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399962)  time: 0.438037  data: 0.201898  max mem: 5431
Epoch: [102/103]  [ 920/1250]  eta: 0:02:24  loss: 2.615268 (2.608391)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399963)  time: 0.426476  data: 0.185693  max mem: 5431
Epoch: [102/103]  [ 930/1250]  eta: 0:02:19  loss: 2.615268 (2.607492)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399963)  time: 0.444420  data: 0.198859  max mem: 5431
Epoch: [102/103]  [ 940/1250]  eta: 0:02:15  loss: 2.480475 (2.606662)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399963)  time: 0.467399  data: 0.228533  max mem: 5431
Epoch: [102/103]  [ 950/1250]  eta: 0:02:10  loss: 2.480475 (2.604687)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399964)  time: 0.428507  data: 0.194267  max mem: 5431
Epoch: [102/103]  [ 960/1250]  eta: 0:02:06  loss: 2.506135 (2.604972)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399964)  time: 0.415090  data: 0.180502  max mem: 5431
Epoch: [102/103]  [ 970/1250]  eta: 0:02:02  loss: 2.494173 (2.604411)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399964)  time: 0.439248  data: 0.196825  max mem: 5431
Epoch: [102/103]  [ 980/1250]  eta: 0:01:58  loss: 2.494173 (2.606047)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399965)  time: 0.472192  data: 0.229342  max mem: 5431
Epoch: [102/103]  [ 990/1250]  eta: 0:01:53  loss: 2.562866 (2.605530)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399965)  time: 0.475335  data: 0.235875  max mem: 5431
Epoch: [102/103]  [1000/1250]  eta: 0:01:49  loss: 2.401483 (2.604616)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399965)  time: 0.450357  data: 0.208643  max mem: 5431
Epoch: [102/103]  [1010/1250]  eta: 0:01:45  loss: 2.514838 (2.603844)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399966)  time: 0.448935  data: 0.207927  max mem: 5431
Epoch: [102/103]  [1020/1250]  eta: 0:01:40  loss: 2.514838 (2.605084)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399966)  time: 0.464610  data: 0.222999  max mem: 5431
Epoch: [102/103]  [1030/1250]  eta: 0:01:36  loss: 2.427848 (2.603067)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399966)  time: 0.443143  data: 0.208395  max mem: 5431
Epoch: [102/103]  [1040/1250]  eta: 0:01:32  loss: 2.427848 (2.604766)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399967)  time: 0.427470  data: 0.192636  max mem: 5431
Epoch: [102/103]  [1050/1250]  eta: 0:01:27  loss: 2.748380 (2.606437)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.448533  data: 0.207822  max mem: 5431
Epoch: [102/103]  [1060/1250]  eta: 0:01:23  loss: 2.655568 (2.608032)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.452779  data: 0.212307  max mem: 5431
Epoch: [102/103]  [1070/1250]  eta: 0:01:18  loss: 2.655568 (2.611185)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.453477  data: 0.213757  max mem: 5431
Epoch: [102/103]  [1080/1250]  eta: 0:01:14  loss: 2.739986 (2.612236)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399968)  time: 0.407524  data: 0.171393  max mem: 5431
Epoch: [102/103]  [1090/1250]  eta: 0:01:10  loss: 2.642470 (2.612868)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399968)  time: 0.403800  data: 0.165910  max mem: 5431
Epoch: [102/103]  [1100/1250]  eta: 0:01:05  loss: 2.493535 (2.612735)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399968)  time: 0.427444  data: 0.186507  max mem: 5431
Epoch: [102/103]  [1110/1250]  eta: 0:01:01  loss: 2.496762 (2.612879)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.430809  data: 0.193390  max mem: 5431
Epoch: [102/103]  [1120/1250]  eta: 0:00:56  loss: 2.545000 (2.611639)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.450918  data: 0.214255  max mem: 5431
Epoch: [102/103]  [1130/1250]  eta: 0:00:52  loss: 2.516608 (2.610493)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.432808  data: 0.194478  max mem: 5431
Epoch: [102/103]  [1140/1250]  eta: 0:00:48  loss: 2.371275 (2.609435)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.434720  data: 0.198639  max mem: 5431
Epoch: [102/103]  [1150/1250]  eta: 0:00:43  loss: 2.343174 (2.609890)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.428019  data: 0.190783  max mem: 5431
Epoch: [102/103]  [1160/1250]  eta: 0:00:39  loss: 2.497396 (2.610026)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.428368  data: 0.184315  max mem: 5431
Epoch: [102/103]  [1170/1250]  eta: 0:00:34  loss: 2.556807 (2.611296)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.446898  data: 0.207495  max mem: 5431
Epoch: [102/103]  [1180/1250]  eta: 0:00:30  loss: 2.601880 (2.611256)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.432307  data: 0.196139  max mem: 5431
Epoch: [102/103]  [1190/1250]  eta: 0:00:26  loss: 2.595187 (2.612325)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.441080  data: 0.201529  max mem: 5431
Epoch: [102/103]  [1200/1250]  eta: 0:00:21  loss: 2.596343 (2.613649)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.448271  data: 0.203571  max mem: 5431
Epoch: [102/103]  [1210/1250]  eta: 0:00:17  loss: 2.517075 (2.612599)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.446912  data: 0.199101  max mem: 5431
Epoch: [102/103]  [1220/1250]  eta: 0:00:13  loss: 2.471655 (2.612583)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.441434  data: 0.200234  max mem: 5431
Epoch: [102/103]  [1230/1250]  eta: 0:00:08  loss: 2.494191 (2.611275)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.440742  data: 0.204114  max mem: 5431
Epoch: [102/103]  [1240/1250]  eta: 0:00:04  loss: 2.459464 (2.610664)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.427300  data: 0.184186  max mem: 5431
Epoch: [102/103]  [1249/1250]  eta: 0:00:00  loss: 2.488594 (2.611064)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.422992  data: 0.180418  max mem: 5431
Epoch: [102/103] Total time: 0:09:07 (0.437766 s / it)
Averaged stats: loss: 2.488594 (2.611064)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)
Training time 0:09:07
Wrote profile results to main_dino.py.lprof
Timer unit: 1e-06 s

Total time: 0 s
File: main_dino.py
Function: collate_function at line 124

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   124                                           @profile
   125                                           def collate_function(batch, additional_arg):
   126                                               process_seed = random.randint(0, 1000000)
   127                                           
   128                                               # Separate the samples and targets
   129                                               samples, targets = zip(*batch)
   130                                           
   131                                               # Apply augmentations to each sample within the batch
   132                                               augmented_samples = []
   133                                               for sample in samples:
   134                                                   augmented_sample = DataAugmentationDINO(additional_arg, sample, process_seed)
   135                                                   augmented_samples.append(augmented_sample)
   136                                           
   137                                               # show_images(augmented_samples, additional_arg.batch_size_per_gpu)
   138                                               
   139                                                   # Decompose data:
   140                                               images =[
   141                                                   torch.empty((len(augmented_samples),3,args.global_scale,args.global_scale)),torch.empty((len(augmented_samples),3,args.global_scale,args.global_scale)),
   142                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   143                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   144                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   145                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   146                                               ]
   147                                               for i in range(len(augmented_samples[0])):
   148                                                   for j in range(len(augmented_samples)):
   149                                                       images[i][j] = augmented_samples[j][i].crop_tensor_normed.detach().clone()
   150                                           
   151                                               corrs = [[None for _ in range(additional_arg.global_crops_number + additional_arg.local_crops_number)] for _ in range(additional_arg.global_crops_number)]
   152                                           
   153                                               # Calculate patch correspondences for the last image in the batch
   154                                               # which is also equal to other images in the batch:
   155                                           
   156                                               for iq in range(additional_arg.global_crops_number):
   157                                                   for v in range(additional_arg.global_crops_number + additional_arg.local_crops_number):
   158                                                       if v == iq:
   159                                                           # we skip cases where student and teacher operate on the same view
   160                                                           continue
   161                                           
   162                                                       corrs[iq][v] = correspondences(augmented_sample[iq], augmented_sample[v])
   163                                               
   164                                               # Return the augmented samples, correspondences, and targets as a batch
   165                                               return images, corrs, targets

Total time: 546.797 s
File: main_dino.py
Function: train_one_epoch at line 451

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   451                                           @profile
   452                                           def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,
   453                                                               optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,
   454                                                               fp16_scaler, args):
   455         1         16.7     16.7      0.0      metric_logger = utils.MetricLogger(delimiter="  ")
   456         1          5.7      5.7      0.0      header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)
   457      1250  244666013.2 195732.8     44.7      for it, (images, corrs, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):
   458                                                   # show_batch_images(data, args.batch_size_per_gpu)
   459                                           
   460                                                   # update weight decay and learning rate according to their schedule
   461      1250      20718.6     16.6      0.0          it = len(data_loader) * epoch + it  # global training iteration
   462      2500       4000.5      1.6      0.0          for i, param_group in enumerate(optimizer.param_groups):
   463      2500       6610.5      2.6      0.0              param_group["lr"] = lr_schedule[it]
   464      1250        728.2      0.6      0.0              if i == 0:  # only the first group is regularized
   465      1250       1766.0      1.4      0.0                  param_group["weight_decay"] = wd_schedule[it]
   466                                           
   467                                                   # # Decompose data:
   468                                                   # images =[
   469                                                   #     torch.empty((len(data),3,args.global_scale,args.global_scale)),torch.empty((len(data),3,args.global_scale,args.global_scale)),
   470                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   471                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   472                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   473                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   474                                                   # ]
   475                                                   # for i in range(len(data[0])):
   476                                                   #     for j in range(len(data)):
   477                                                   #         images[i][j] = data[j][i].crop_tensor_normed.detach().clone()
   478                                           
   479                                                   # move images to gpu
   480      1250     290332.0    232.3      0.1          images = [im.cuda(non_blocking=True) for im in images]
   481                                                   # teacher and student forward passes + compute dino loss
   482      1250      62367.3     49.9      0.0          with torch.cuda.amp.autocast(fp16_scaler is not None):
   483      1250   20833701.9  16667.0      3.8              teacher_output = teacher(images[:2], args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)  # only the 2 global views pass through the teacher
   484      1250   74752043.3  59801.6     13.7              student_output = student(images, args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)
   485      1250   24378305.4  19502.6      4.5              loss = dino_loss(student_output, teacher_output, corrs, epoch)
   486                                           
   487      1250      60006.6     48.0      0.0          if not math.isfinite(loss.item()):
   488                                                       print("Loss is {}, stopping training".format(loss.item()), force=True)
   489                                                       sys.exit(1)
   490                                           
   491                                                   # student update
   492      1250    2011898.8   1609.5      0.4          optimizer.zero_grad()
   493      1250       9478.9      7.6      0.0          param_norms = None
   494      1250        499.7      0.4      0.0          if fp16_scaler is None:
   495                                                       loss.backward()
   496                                                       if args.clip_grad:
   497                                                           param_norms = utils.clip_gradients(student, args.clip_grad)
   498                                                       utils.cancel_gradients_last_layer(epoch, student,
   499                                                                                         args.freeze_last_layer)
   500                                                       optimizer.step()
   501                                                   else:
   502      1250   89796326.5  71837.1     16.4              fp16_scaler.scale(loss).backward()
   503      1250       5574.4      4.5      0.0              if args.clip_grad:
   504      1250    1799334.8   1439.5      0.3                  fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
   505      1250   54578561.9  43662.8     10.0                  param_norms = utils.clip_gradients(student, args.clip_grad)
   506      1250       5166.1      4.1      0.0              utils.cancel_gradients_last_layer(epoch, student,
   507      1250       1527.0      1.2      0.0                                                args.freeze_last_layer)
   508      1250   21626409.5  17301.1      4.0              fp16_scaler.step(optimizer)
   509      1250      71507.2     57.2      0.0              fp16_scaler.update()
   510                                           
   511                                                   # EMA update for the teacher
   512      1250      19182.3     15.3      0.0          with torch.no_grad():
   513      1250       6990.4      5.6      0.0              m = momentum_schedule[it]  # momentum parameter
   514    197500    3787586.5     19.2      0.7              for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):
   515    197500    7796326.3     39.5      1.4                  param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
   516                                           
   517                                                   # logging
   518      1250     110376.7     88.3      0.0          torch.cuda.synchronize()
   519      1250      76038.1     60.8      0.0          metric_logger.update(loss=loss.item())
   520      1250      10231.2      8.2      0.0          metric_logger.update(lr=optimizer.param_groups[0]["lr"])
   521      1250       6458.2      5.2      0.0          metric_logger.update(wd=optimizer.param_groups[0]["weight_decay"])
   522                                               # gather the stats from all processes
   523         1        742.7    742.7      0.0      metric_logger.synchronize_between_processes()
   524         1        252.0    252.0      0.0      print("Averaged stats:", metric_logger)
   525         1          8.8      8.8      0.0      return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


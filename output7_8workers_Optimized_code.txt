Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: bf1c7d901190bc596b3bb9c04d34e6782f0c10f6, status: has uncommited changes, branch: QCRI_Cifar10_same_batch_augmentation_correspondence_on_CPU

arch: vit_tiny
batch_size_per_gpu: 40
clip_grad: 3.0
data_path: /home/alij/Datasets/Cifar10/train
dist_url: env://
drop_path_rate: 0.1
epochs: 113
freeze_last_layer: 1
global_crops_number: 2
global_crops_scale: (0.4, 1.0)
global_scale: 224
gpu: 0
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
local_scale: 96
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
norm_last_layer: True
num_workers: 8
optimizer: adamw
out_dim: 1000
output_dir: /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu
patch_size: 16
rank: 0
saveckp_freq: 20
seed: 0
teacher_temp: 0.04
use_bn_in_head: False
use_fp16: True
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 50000 images.
Student and Teacher are built: they are both vit_tiny network.
Loss, optimizer and schedulers ready.
Found checkpoint at /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth
=> loaded 'student' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'fp16_scaler' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
Starting DINO training !
Epoch: [112/113]  [   0/1250]  eta: 1:52:36  loss: 2.655082 (2.655082)  lr: 0.000001 (0.000001)  wd: 0.399930 (0.399930)  time: 5.405464  data: 2.072614  max mem: 4249
Epoch: [112/113]  [  10/1250]  eta: 0:13:49  loss: 2.558234 (2.559933)  lr: 0.000001 (0.000001)  wd: 0.399931 (0.399931)  time: 0.668772  data: 0.188562  max mem: 4347
Epoch: [112/113]  [  20/1250]  eta: 0:09:05  loss: 2.453571 (2.486230)  lr: 0.000001 (0.000001)  wd: 0.399932 (0.399932)  time: 0.195203  data: 0.000185  max mem: 4348
Epoch: [112/113]  [  30/1250]  eta: 0:07:28  loss: 2.350012 (2.538663)  lr: 0.000001 (0.000001)  wd: 0.399933 (0.399932)  time: 0.201777  data: 0.000231  max mem: 4357
Epoch: [112/113]  [  40/1250]  eta: 0:06:37  loss: 2.591907 (2.549171)  lr: 0.000001 (0.000001)  wd: 0.399934 (0.399933)  time: 0.208207  data: 0.000240  max mem: 4357
Epoch: [112/113]  [  50/1250]  eta: 0:06:05  loss: 2.544284 (2.549221)  lr: 0.000001 (0.000001)  wd: 0.399935 (0.399933)  time: 0.206636  data: 0.000213  max mem: 4357
Epoch: [112/113]  [  60/1250]  eta: 0:05:48  loss: 2.511596 (2.546914)  lr: 0.000001 (0.000001)  wd: 0.399936 (0.399934)  time: 0.220678  data: 0.000227  max mem: 4357
Epoch: [112/113]  [  70/1250]  eta: 0:05:49  loss: 2.616767 (2.572799)  lr: 0.000001 (0.000001)  wd: 0.399937 (0.399934)  time: 0.273764  data: 0.000269  max mem: 4357
Epoch: [112/113]  [  80/1250]  eta: 0:05:47  loss: 2.722703 (2.585276)  lr: 0.000001 (0.000001)  wd: 0.399938 (0.399935)  time: 0.309748  data: 0.000353  max mem: 4357
Epoch: [112/113]  [  90/1250]  eta: 0:05:33  loss: 2.556981 (2.587327)  lr: 0.000001 (0.000001)  wd: 0.399939 (0.399935)  time: 0.257575  data: 0.000311  max mem: 4357
Epoch: [112/113]  [ 100/1250]  eta: 0:05:20  loss: 2.505027 (2.587459)  lr: 0.000001 (0.000001)  wd: 0.399940 (0.399936)  time: 0.202021  data: 0.000177  max mem: 4366
Epoch: [112/113]  [ 110/1250]  eta: 0:05:08  loss: 2.617311 (2.589616)  lr: 0.000001 (0.000001)  wd: 0.399941 (0.399936)  time: 0.195819  data: 0.000179  max mem: 4366
Epoch: [112/113]  [ 120/1250]  eta: 0:04:58  loss: 2.551218 (2.586934)  lr: 0.000001 (0.000001)  wd: 0.399942 (0.399937)  time: 0.194637  data: 0.000187  max mem: 4374
Epoch: [112/113]  [ 130/1250]  eta: 0:04:50  loss: 2.487613 (2.585175)  lr: 0.000001 (0.000001)  wd: 0.399943 (0.399937)  time: 0.195031  data: 0.000177  max mem: 4374
Epoch: [112/113]  [ 140/1250]  eta: 0:04:42  loss: 2.508110 (2.600188)  lr: 0.000001 (0.000001)  wd: 0.399944 (0.399938)  time: 0.195894  data: 0.000172  max mem: 4374
Epoch: [112/113]  [ 150/1250]  eta: 0:04:35  loss: 2.652153 (2.609935)  lr: 0.000001 (0.000001)  wd: 0.399945 (0.399938)  time: 0.195070  data: 0.000166  max mem: 4374
Epoch: [112/113]  [ 160/1250]  eta: 0:04:29  loss: 2.652153 (2.618602)  lr: 0.000001 (0.000001)  wd: 0.399946 (0.399939)  time: 0.193360  data: 0.000166  max mem: 4374
Epoch: [112/113]  [ 170/1250]  eta: 0:04:23  loss: 2.639965 (2.619447)  lr: 0.000001 (0.000001)  wd: 0.399947 (0.399939)  time: 0.193942  data: 0.000160  max mem: 4374
Epoch: [112/113]  [ 180/1250]  eta: 0:04:18  loss: 2.635637 (2.618583)  lr: 0.000001 (0.000001)  wd: 0.399948 (0.399940)  time: 0.194253  data: 0.000156  max mem: 4374
Epoch: [112/113]  [ 190/1250]  eta: 0:04:14  loss: 2.416334 (2.607369)  lr: 0.000001 (0.000001)  wd: 0.399949 (0.399940)  time: 0.201004  data: 0.000154  max mem: 4374
Epoch: [112/113]  [ 200/1250]  eta: 0:04:09  loss: 2.374177 (2.598751)  lr: 0.000001 (0.000001)  wd: 0.399950 (0.399941)  time: 0.202395  data: 0.000155  max mem: 4374
Epoch: [112/113]  [ 210/1250]  eta: 0:04:04  loss: 2.595859 (2.603326)  lr: 0.000001 (0.000001)  wd: 0.399951 (0.399941)  time: 0.195373  data: 0.000162  max mem: 4374
Epoch: [112/113]  [ 220/1250]  eta: 0:04:00  loss: 2.656462 (2.596396)  lr: 0.000001 (0.000001)  wd: 0.399952 (0.399942)  time: 0.194843  data: 0.000159  max mem: 4374
Epoch: [112/113]  [ 230/1250]  eta: 0:03:56  loss: 2.452466 (2.593428)  lr: 0.000001 (0.000001)  wd: 0.399953 (0.399942)  time: 0.197394  data: 0.000157  max mem: 4374
Epoch: [112/113]  [ 240/1250]  eta: 0:03:52  loss: 2.452466 (2.588465)  lr: 0.000001 (0.000001)  wd: 0.399954 (0.399943)  time: 0.198234  data: 0.000155  max mem: 4374
Epoch: [112/113]  [ 250/1250]  eta: 0:03:49  loss: 2.354049 (2.582575)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399943)  time: 0.196932  data: 0.000152  max mem: 4374
Epoch: [112/113]  [ 260/1250]  eta: 0:03:49  loss: 2.381076 (2.576041)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399944)  time: 0.244391  data: 0.000218  max mem: 4374
Epoch: [112/113]  [ 270/1250]  eta: 0:03:50  loss: 2.401759 (2.577369)  lr: 0.000001 (0.000001)  wd: 0.399956 (0.399944)  time: 0.307223  data: 0.000312  max mem: 4374
Epoch: [112/113]  [ 280/1250]  eta: 0:03:48  loss: 2.544189 (2.573281)  lr: 0.000001 (0.000001)  wd: 0.399957 (0.399945)  time: 0.287031  data: 0.000292  max mem: 4374
Epoch: [112/113]  [ 290/1250]  eta: 0:03:44  loss: 2.544545 (2.576510)  lr: 0.000001 (0.000001)  wd: 0.399958 (0.399945)  time: 0.222679  data: 0.000208  max mem: 4374
Epoch: [112/113]  [ 300/1250]  eta: 0:03:41  loss: 2.544545 (2.578434)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399946)  time: 0.198109  data: 0.000178  max mem: 4374
Epoch: [112/113]  [ 310/1250]  eta: 0:03:38  loss: 2.687188 (2.586463)  lr: 0.000001 (0.000001)  wd: 0.399960 (0.399946)  time: 0.200841  data: 0.000164  max mem: 4374
Epoch: [112/113]  [ 320/1250]  eta: 0:03:34  loss: 2.690569 (2.585677)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399947)  time: 0.197782  data: 0.000162  max mem: 4374
Epoch: [112/113]  [ 330/1250]  eta: 0:03:31  loss: 2.535305 (2.584529)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399947)  time: 0.197105  data: 0.000181  max mem: 4374
Epoch: [112/113]  [ 340/1250]  eta: 0:03:28  loss: 2.535305 (2.583758)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399948)  time: 0.197552  data: 0.000188  max mem: 4374
Epoch: [112/113]  [ 350/1250]  eta: 0:03:25  loss: 2.436675 (2.578136)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399948)  time: 0.197592  data: 0.000193  max mem: 4374
Epoch: [112/113]  [ 360/1250]  eta: 0:03:22  loss: 2.383181 (2.574285)  lr: 0.000001 (0.000001)  wd: 0.399964 (0.399949)  time: 0.199759  data: 0.000200  max mem: 4374
Epoch: [112/113]  [ 370/1250]  eta: 0:03:19  loss: 2.551365 (2.579344)  lr: 0.000001 (0.000001)  wd: 0.399965 (0.399949)  time: 0.200315  data: 0.000186  max mem: 4374
Epoch: [112/113]  [ 380/1250]  eta: 0:03:16  loss: 2.751737 (2.585668)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399949)  time: 0.198574  data: 0.000163  max mem: 4374
Epoch: [112/113]  [ 390/1250]  eta: 0:03:13  loss: 2.595899 (2.584823)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399950)  time: 0.197553  data: 0.000173  max mem: 4374
Epoch: [112/113]  [ 400/1250]  eta: 0:03:10  loss: 2.526997 (2.585961)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399950)  time: 0.197626  data: 0.000192  max mem: 4374
Epoch: [112/113]  [ 410/1250]  eta: 0:03:08  loss: 2.530573 (2.586757)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399951)  time: 0.198264  data: 0.000187  max mem: 4374
Epoch: [112/113]  [ 420/1250]  eta: 0:03:05  loss: 2.664652 (2.588457)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399951)  time: 0.199240  data: 0.000183  max mem: 4374
Epoch: [112/113]  [ 430/1250]  eta: 0:03:02  loss: 2.664652 (2.593118)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399952)  time: 0.199414  data: 0.000172  max mem: 4374
Epoch: [112/113]  [ 440/1250]  eta: 0:02:59  loss: 2.663018 (2.598226)  lr: 0.000001 (0.000001)  wd: 0.399970 (0.399952)  time: 0.199086  data: 0.000174  max mem: 4374
Epoch: [112/113]  [ 450/1250]  eta: 0:02:58  loss: 2.445851 (2.596463)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399952)  time: 0.244526  data: 0.000265  max mem: 4374
Epoch: [112/113]  [ 460/1250]  eta: 0:02:58  loss: 2.467366 (2.595960)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399953)  time: 0.299953  data: 0.000322  max mem: 4374
Epoch: [112/113]  [ 470/1250]  eta: 0:02:55  loss: 2.484704 (2.594062)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399953)  time: 0.268951  data: 0.000255  max mem: 4374
Epoch: [112/113]  [ 480/1250]  eta: 0:02:53  loss: 2.592747 (2.599266)  lr: 0.000001 (0.000001)  wd: 0.399973 (0.399954)  time: 0.213251  data: 0.000188  max mem: 4374
Epoch: [112/113]  [ 490/1250]  eta: 0:02:50  loss: 2.676176 (2.600635)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399954)  time: 0.199567  data: 0.000172  max mem: 4374
Epoch: [112/113]  [ 500/1250]  eta: 0:02:48  loss: 2.486104 (2.600315)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399955)  time: 0.199426  data: 0.000185  max mem: 4374
Epoch: [112/113]  [ 510/1250]  eta: 0:02:45  loss: 2.493376 (2.602236)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399955)  time: 0.198496  data: 0.000189  max mem: 4374
Epoch: [112/113]  [ 520/1250]  eta: 0:02:42  loss: 2.636541 (2.603101)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399955)  time: 0.198221  data: 0.000185  max mem: 4374
Epoch: [112/113]  [ 530/1250]  eta: 0:02:40  loss: 2.600482 (2.603956)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399956)  time: 0.198624  data: 0.000176  max mem: 4374
Epoch: [112/113]  [ 540/1250]  eta: 0:02:37  loss: 2.506999 (2.602012)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399956)  time: 0.199718  data: 0.000170  max mem: 4374
Epoch: [112/113]  [ 550/1250]  eta: 0:02:35  loss: 2.601104 (2.605798)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399957)  time: 0.200678  data: 0.000178  max mem: 4374
Epoch: [112/113]  [ 560/1250]  eta: 0:02:32  loss: 2.601104 (2.604600)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399957)  time: 0.200307  data: 0.000195  max mem: 4374
Epoch: [112/113]  [ 570/1250]  eta: 0:02:30  loss: 2.330691 (2.600798)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399957)  time: 0.199616  data: 0.000198  max mem: 4374
Epoch: [112/113]  [ 580/1250]  eta: 0:02:27  loss: 2.419188 (2.601567)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399958)  time: 0.198918  data: 0.000164  max mem: 4374
Epoch: [112/113]  [ 590/1250]  eta: 0:02:25  loss: 2.442317 (2.600538)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399958)  time: 0.201439  data: 0.000165  max mem: 4374
Epoch: [112/113]  [ 600/1250]  eta: 0:02:23  loss: 2.530987 (2.601809)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399958)  time: 0.203283  data: 0.000192  max mem: 4374
Epoch: [112/113]  [ 610/1250]  eta: 0:02:20  loss: 2.578288 (2.602074)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399959)  time: 0.202757  data: 0.000186  max mem: 4374
Epoch: [112/113]  [ 620/1250]  eta: 0:02:18  loss: 2.577914 (2.602936)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399959)  time: 0.203699  data: 0.000178  max mem: 4374
Epoch: [112/113]  [ 630/1250]  eta: 0:02:15  loss: 2.541595 (2.601032)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399960)  time: 0.204142  data: 0.000178  max mem: 4374
Epoch: [112/113]  [ 640/1250]  eta: 0:02:14  loss: 2.541595 (2.602812)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399960)  time: 0.262267  data: 0.000255  max mem: 4374
Epoch: [112/113]  [ 650/1250]  eta: 0:02:13  loss: 2.565244 (2.602538)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399960)  time: 0.316332  data: 0.000325  max mem: 4374
Epoch: [112/113]  [ 660/1250]  eta: 0:02:11  loss: 2.526846 (2.602554)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399961)  time: 0.265479  data: 0.000306  max mem: 4374
Epoch: [112/113]  [ 670/1250]  eta: 0:02:08  loss: 2.531440 (2.604011)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399961)  time: 0.209216  data: 0.000234  max mem: 4374
Epoch: [112/113]  [ 680/1250]  eta: 0:02:06  loss: 2.540125 (2.604909)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399961)  time: 0.198703  data: 0.000178  max mem: 4374
Epoch: [112/113]  [ 690/1250]  eta: 0:02:03  loss: 2.503887 (2.602720)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399962)  time: 0.198508  data: 0.000181  max mem: 4374
Epoch: [112/113]  [ 700/1250]  eta: 0:02:01  loss: 2.508279 (2.603300)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399962)  time: 0.199945  data: 0.000189  max mem: 4374
Epoch: [112/113]  [ 710/1250]  eta: 0:01:59  loss: 2.604153 (2.602809)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399962)  time: 0.198997  data: 0.000192  max mem: 4374
Epoch: [112/113]  [ 720/1250]  eta: 0:01:56  loss: 2.654135 (2.604574)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399963)  time: 0.198861  data: 0.000190  max mem: 4374
Epoch: [112/113]  [ 730/1250]  eta: 0:01:54  loss: 2.396728 (2.600516)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399963)  time: 0.201203  data: 0.000183  max mem: 4374
Epoch: [112/113]  [ 740/1250]  eta: 0:01:52  loss: 2.378092 (2.599430)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399963)  time: 0.200775  data: 0.000189  max mem: 4374
Epoch: [112/113]  [ 750/1250]  eta: 0:01:49  loss: 2.408052 (2.598057)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399964)  time: 0.198922  data: 0.000198  max mem: 4374
Epoch: [112/113]  [ 760/1250]  eta: 0:01:47  loss: 2.460967 (2.599450)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399964)  time: 0.200587  data: 0.000184  max mem: 4374
Epoch: [112/113]  [ 770/1250]  eta: 0:01:45  loss: 2.578930 (2.601081)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399964)  time: 0.201516  data: 0.000168  max mem: 4374
Epoch: [112/113]  [ 780/1250]  eta: 0:01:42  loss: 2.573795 (2.599673)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399965)  time: 0.200530  data: 0.000144  max mem: 4374
Epoch: [112/113]  [ 790/1250]  eta: 0:01:40  loss: 2.565911 (2.600540)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399965)  time: 0.199630  data: 0.000163  max mem: 4374
Epoch: [112/113]  [ 800/1250]  eta: 0:01:38  loss: 2.540083 (2.599150)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399965)  time: 0.198696  data: 0.000192  max mem: 4374
Epoch: [112/113]  [ 810/1250]  eta: 0:01:35  loss: 2.492090 (2.599194)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399966)  time: 0.200057  data: 0.000187  max mem: 4374
Epoch: [112/113]  [ 820/1250]  eta: 0:01:33  loss: 2.569738 (2.599471)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399966)  time: 0.206318  data: 0.000171  max mem: 4374
Epoch: [112/113]  [ 830/1250]  eta: 0:01:32  loss: 2.589839 (2.600147)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399966)  time: 0.264387  data: 0.000257  max mem: 4374
Epoch: [112/113]  [ 840/1250]  eta: 0:01:30  loss: 2.503922 (2.597591)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399967)  time: 0.313818  data: 0.000326  max mem: 4374
Epoch: [112/113]  [ 850/1250]  eta: 0:01:28  loss: 2.453728 (2.596981)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399967)  time: 0.261688  data: 0.000280  max mem: 4374
Epoch: [112/113]  [ 860/1250]  eta: 0:01:25  loss: 2.498382 (2.598298)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399967)  time: 0.212023  data: 0.000227  max mem: 4374
Epoch: [112/113]  [ 870/1250]  eta: 0:01:23  loss: 2.512142 (2.598434)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399968)  time: 0.204140  data: 0.000190  max mem: 4374
Epoch: [112/113]  [ 880/1250]  eta: 0:01:21  loss: 2.544054 (2.597804)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399968)  time: 0.197145  data: 0.000196  max mem: 4374
Epoch: [112/113]  [ 890/1250]  eta: 0:01:18  loss: 2.516945 (2.597119)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399968)  time: 0.197658  data: 0.000210  max mem: 4374
Epoch: [112/113]  [ 900/1250]  eta: 0:01:16  loss: 2.393722 (2.596407)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399968)  time: 0.199206  data: 0.000189  max mem: 4374
Epoch: [112/113]  [ 910/1250]  eta: 0:01:14  loss: 2.415565 (2.596940)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399969)  time: 0.198544  data: 0.000177  max mem: 4374
Epoch: [112/113]  [ 920/1250]  eta: 0:01:12  loss: 2.560603 (2.596934)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399969)  time: 0.197484  data: 0.000196  max mem: 4374
Epoch: [112/113]  [ 930/1250]  eta: 0:01:09  loss: 2.609813 (2.599356)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399969)  time: 0.198018  data: 0.000208  max mem: 4374
Epoch: [112/113]  [ 940/1250]  eta: 0:01:07  loss: 2.735846 (2.599961)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399970)  time: 0.198283  data: 0.000212  max mem: 4374
Epoch: [112/113]  [ 950/1250]  eta: 0:01:05  loss: 2.593750 (2.599502)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399970)  time: 0.199728  data: 0.000204  max mem: 4374
Epoch: [112/113]  [ 960/1250]  eta: 0:01:03  loss: 2.599569 (2.600295)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399970)  time: 0.199503  data: 0.000187  max mem: 4374
Epoch: [112/113]  [ 970/1250]  eta: 0:01:00  loss: 2.571459 (2.599994)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399970)  time: 0.198271  data: 0.000191  max mem: 4374
Epoch: [112/113]  [ 980/1250]  eta: 0:00:58  loss: 2.565801 (2.600431)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399971)  time: 0.198636  data: 0.000202  max mem: 4374
Epoch: [112/113]  [ 990/1250]  eta: 0:00:56  loss: 2.749510 (2.602136)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399971)  time: 0.198738  data: 0.000200  max mem: 4374
Epoch: [112/113]  [1000/1250]  eta: 0:00:54  loss: 2.535966 (2.600909)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399971)  time: 0.200811  data: 0.000191  max mem: 4374
Epoch: [112/113]  [1010/1250]  eta: 0:00:52  loss: 2.472710 (2.600285)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399971)  time: 0.204624  data: 0.000175  max mem: 4374
Epoch: [112/113]  [1020/1250]  eta: 0:00:50  loss: 2.519222 (2.600527)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399972)  time: 0.261824  data: 0.000290  max mem: 4374
Epoch: [112/113]  [1030/1250]  eta: 0:00:48  loss: 2.689671 (2.601779)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399972)  time: 0.318993  data: 0.000342  max mem: 4374
Epoch: [112/113]  [1040/1250]  eta: 0:00:45  loss: 2.651908 (2.602523)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399972)  time: 0.262726  data: 0.000247  max mem: 4374
Epoch: [112/113]  [1050/1250]  eta: 0:00:43  loss: 2.532892 (2.603375)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399973)  time: 0.200772  data: 0.000191  max mem: 4374
Epoch: [112/113]  [1060/1250]  eta: 0:00:41  loss: 2.658526 (2.604671)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399973)  time: 0.196831  data: 0.000186  max mem: 4374
Epoch: [112/113]  [1070/1250]  eta: 0:00:39  loss: 2.624909 (2.604246)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399973)  time: 0.199795  data: 0.000200  max mem: 4374
Epoch: [112/113]  [1080/1250]  eta: 0:00:37  loss: 2.547515 (2.603125)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399973)  time: 0.200379  data: 0.000178  max mem: 4374
Epoch: [112/113]  [1090/1250]  eta: 0:00:34  loss: 2.533696 (2.604720)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399973)  time: 0.196885  data: 0.000167  max mem: 4374
Epoch: [112/113]  [1100/1250]  eta: 0:00:32  loss: 2.607900 (2.604190)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399974)  time: 0.196733  data: 0.000193  max mem: 4374
Epoch: [112/113]  [1110/1250]  eta: 0:00:30  loss: 2.453404 (2.602246)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399974)  time: 0.197164  data: 0.000217  max mem: 4374
Epoch: [112/113]  [1120/1250]  eta: 0:00:28  loss: 2.351681 (2.600345)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399974)  time: 0.196967  data: 0.000204  max mem: 4374
Epoch: [112/113]  [1130/1250]  eta: 0:00:26  loss: 2.490924 (2.598938)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399974)  time: 0.198276  data: 0.000197  max mem: 4374
Epoch: [112/113]  [1140/1250]  eta: 0:00:23  loss: 2.524881 (2.597940)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399975)  time: 0.199354  data: 0.000213  max mem: 4374
Epoch: [112/113]  [1150/1250]  eta: 0:00:21  loss: 2.541764 (2.600821)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399975)  time: 0.198629  data: 0.000220  max mem: 4374
Epoch: [112/113]  [1160/1250]  eta: 0:00:19  loss: 2.682010 (2.600234)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399975)  time: 0.197797  data: 0.000197  max mem: 4374
Epoch: [112/113]  [1170/1250]  eta: 0:00:17  loss: 2.682010 (2.601682)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399975)  time: 0.198355  data: 0.000192  max mem: 4374
Epoch: [112/113]  [1180/1250]  eta: 0:00:15  loss: 2.718012 (2.602424)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399975)  time: 0.199057  data: 0.000193  max mem: 4374
Epoch: [112/113]  [1190/1250]  eta: 0:00:12  loss: 2.601255 (2.603317)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399976)  time: 0.198554  data: 0.000162  max mem: 4374
Epoch: [112/113]  [1200/1250]  eta: 0:00:10  loss: 2.542156 (2.602836)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399976)  time: 0.207307  data: 0.000167  max mem: 4374
Epoch: [112/113]  [1210/1250]  eta: 0:00:08  loss: 2.505085 (2.602710)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399976)  time: 0.259266  data: 0.000294  max mem: 4374
Epoch: [112/113]  [1220/1250]  eta: 0:00:06  loss: 2.748251 (2.604261)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399976)  time: 0.300660  data: 0.000396  max mem: 4374
Epoch: [112/113]  [1230/1250]  eta: 0:00:04  loss: 2.662654 (2.604550)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399976)  time: 0.259675  data: 0.000297  max mem: 4374
Epoch: [112/113]  [1240/1250]  eta: 0:00:02  loss: 2.662654 (2.607063)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399977)  time: 0.208305  data: 0.000203  max mem: 4374
Epoch: [112/113]  [1249/1250]  eta: 0:00:00  loss: 2.604241 (2.607376)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399977)  time: 0.194372  data: 0.000139  max mem: 4374
Epoch: [112/113] Total time: 0:04:31 (0.217435 s / it)
Averaged stats: loss: 2.604241 (2.607376)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399977)
Training time 0:04:32
Wrote profile results to main_dino.py.lprof
Timer unit: 1e-06 s

Total time: 0 s
File: main_dino.py
Function: collate_function at line 125

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   125                                           @profile
   126                                           def collate_function(batch, additional_arg):
   127                                               process_seed = random.randint(0, 1000000)
   128                                           
   129                                               # Separate the samples and targets
   130                                               samples, targets = zip(*batch)
   131                                           
   132                                               # Apply augmentations to each sample within the batch
   133                                               augmented_samples = []
   134                                           
   135                                               for i, sample in enumerate(samples):
   136                                                   augmented_sample = DataAugmentationDINO(additional_arg, sample, process_seed)
   137                                                   augmented_samples.append(augmented_sample)
   138                                               # show_images(augmented_samples, additional_arg.batch_size_per_gpu)
   139                                               
   140                                               # Decompose data:
   141                                               images =[
   142                                                   torch.empty((len(augmented_samples),3,args.global_scale,args.global_scale)),torch.empty((len(augmented_samples),3,args.global_scale,args.global_scale)),
   143                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   144                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   145                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   146                                                   torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),torch.empty((len(augmented_samples),3,args.local_scale,args.local_scale)),
   147                                               ]
   148                                               for i in range(len(augmented_samples[0])):
   149                                                   for j in range(len(augmented_samples)):
   150                                                       images[i][j] = augmented_samples[j][i].crop_tensor_normed.detach().clone()
   151                                           
   152                                               corrs = [[None for _ in range(additional_arg.global_crops_number + additional_arg.local_crops_number)] for _ in range(additional_arg.global_crops_number)]
   153                                           
   154                                               # Calculate patch correspondences for the last image in the batch
   155                                               # which is also equal to other images in the batch:
   156                                           
   157                                               for iq in range(additional_arg.global_crops_number):
   158                                                   for v in range(additional_arg.global_crops_number + additional_arg.local_crops_number):
   159                                                       if v == iq:
   160                                                           # we skip cases where student and teacher operate on the same view
   161                                                           continue
   162                                           
   163                                                       corrs[iq][v] = correspondences(augmented_sample[iq], augmented_sample[v])
   164                                               
   165                                               # Return the augmented samples, correspondences, and targets as a batch
   166                                               return images, corrs, targets

Total time: 271.422 s
File: main_dino.py
Function: train_one_epoch at line 452

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   452                                           @profile
   453                                           def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,
   454                                                               optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,
   455                                                               fp16_scaler, args):
   456         1         18.2     18.2      0.0      metric_logger = utils.MetricLogger(delimiter="  ")
   457         1          4.6      4.6      0.0      header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)
   458      1250    3422574.0   2738.1      1.3      for it, (images, corrs, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):
   459                                                   # show_batch_images(data, args.batch_size_per_gpu)
   460                                           
   461                                                   # update weight decay and learning rate according to their schedule
   462      1250      15825.8     12.7      0.0          it = len(data_loader) * epoch + it  # global training iteration
   463      2500       3304.6      1.3      0.0          for i, param_group in enumerate(optimizer.param_groups):
   464      2500       5101.9      2.0      0.0              param_group["lr"] = lr_schedule[it]
   465      1250        538.9      0.4      0.0              if i == 0:  # only the first group is regularized
   466      1250       1570.3      1.3      0.0                  param_group["weight_decay"] = wd_schedule[it]
   467                                           
   468                                                   # move images to gpu
   469      1250     235552.9    188.4      0.1          images = [im.cuda(non_blocking=True) for im in images]
   470                                                   # teacher and student forward passes + compute dino loss
   471      1250      53627.2     42.9      0.0          with torch.cuda.amp.autocast(fp16_scaler is not None):
   472      1250   19677370.7  15741.9      7.2              teacher_output = teacher(images[:2], args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)  # only the 2 global views pass through the teacher
   473      1250   44545587.2  35636.5     16.4              student_output1, student_output2 = student(images, args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)
   474      1250   32458613.9  25966.9     12.0              loss = dino_loss(student_output1, student_output2, teacher_output, corrs, epoch)
   475                                           
   476      1250      63350.1     50.7      0.0          if not math.isfinite(loss.item()):
   477                                                       print("Loss is {}, stopping training".format(loss.item()), force=True)
   478                                                       sys.exit(1)
   479                                           
   480                                                   # student update
   481      1250    1929698.8   1543.8      0.7          optimizer.zero_grad()
   482      1250       9161.9      7.3      0.0          param_norms = None
   483      1250        543.1      0.4      0.0          if fp16_scaler is None:
   484                                                       loss.backward()
   485                                                       if args.clip_grad:
   486                                                           param_norms = utils.clip_gradients(student, args.clip_grad)
   487                                                       utils.cancel_gradients_last_layer(epoch, student,
   488                                                                                         args.freeze_last_layer)
   489                                                       optimizer.step()
   490                                                   else:
   491      1250   79930827.6  63944.7     29.4              fp16_scaler.scale(loss).backward()
   492      1250       7089.6      5.7      0.0              if args.clip_grad:
   493      1250    2971074.9   2376.9      1.1                  fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
   494      1250   53636935.6  42909.5     19.8                  param_norms = utils.clip_gradients(student, args.clip_grad)
   495      1250       4582.6      3.7      0.0              utils.cancel_gradients_last_layer(epoch, student,
   496      1250       1835.9      1.5      0.0                                                args.freeze_last_layer)
   497      1250   21315840.0  17052.7      7.9              fp16_scaler.step(optimizer)
   498      1250      71009.1     56.8      0.0              fp16_scaler.update()
   499                                           
   500                                                   # EMA update for the teacher
   501      1250      16844.2     13.5      0.0          with torch.no_grad():
   502      1250       4812.7      3.9      0.0              m = momentum_schedule[it]  # momentum parameter
   503    197500    3258748.4     16.5      1.2              for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):
   504    197500    7586742.5     38.4      2.8                  param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
   505                                           
   506                                                   # logging
   507      1250     101996.3     81.6      0.0          torch.cuda.synchronize()
   508      1250      74565.8     59.7      0.0          metric_logger.update(loss=loss.item())
   509      1250       8851.2      7.1      0.0          metric_logger.update(lr=optimizer.param_groups[0]["lr"])
   510      1250       5753.8      4.6      0.0          metric_logger.update(wd=optimizer.param_groups[0]["weight_decay"])
   511                                               # gather the stats from all processes
   512         1       1325.6   1325.6      0.0      metric_logger.synchronize_between_processes()
   513         1        475.6    475.6      0.0      print("Averaged stats:", metric_logger)
   514         1         14.9     14.9      0.0      return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


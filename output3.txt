Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: 28fcf3dd2d6a975b9f9ca5b35a0244e7ec6ad620, status: has uncommited changes, branch: QCRI_Cifar10_same_batch_augmentation_correspondence_on_CPU

arch: vit_tiny
batch_size_per_gpu: 40
clip_grad: 3.0
data_path: /home/alij/Datasets/Cifar10/train
dist_url: env://
drop_path_rate: 0.1
epochs: 106
freeze_last_layer: 1
global_crops_number: 2
global_crops_scale: (0.4, 1.0)
global_scale: 224
gpu: 0
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
local_scale: 96
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
norm_last_layer: True
num_workers: 4
optimizer: adamw
out_dim: 1000
output_dir: /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu
patch_size: 16
rank: 0
saveckp_freq: 20
seed: 0
teacher_temp: 0.04
use_bn_in_head: False
use_fp16: True
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 50000 images.
Student and Teacher are built: they are both vit_tiny network.
Loss, optimizer and schedulers ready.
Found checkpoint at /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth
=> loaded 'student' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'fp16_scaler' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
Starting DINO training !
Epoch: [105/106]  [   0/1250]  eta: 1:41:15  loss: 2.520795 (2.520795)  lr: 0.000001 (0.000001)  wd: 0.399921 (0.399921)  time: 4.860267  data: 2.419487  max mem: 5312
Epoch: [105/106]  [  10/1250]  eta: 0:14:05  loss: 2.481352 (2.490579)  lr: 0.000001 (0.000001)  wd: 0.399922 (0.399922)  time: 0.682105  data: 0.255168  max mem: 5407
Epoch: [105/106]  [  20/1250]  eta: 0:11:35  loss: 2.444771 (2.564063)  lr: 0.000001 (0.000001)  wd: 0.399922 (0.399922)  time: 0.350929  data: 0.127425  max mem: 5409
Epoch: [105/106]  [  30/1250]  eta: 0:11:05  loss: 2.430107 (2.515666)  lr: 0.000001 (0.000001)  wd: 0.399923 (0.399923)  time: 0.470497  data: 0.248553  max mem: 5414
Epoch: [105/106]  [  40/1250]  eta: 0:10:40  loss: 2.480579 (2.559968)  lr: 0.000001 (0.000001)  wd: 0.399925 (0.399923)  time: 0.490682  data: 0.268031  max mem: 5414
Epoch: [105/106]  [  50/1250]  eta: 0:10:18  loss: 2.527412 (2.550936)  lr: 0.000001 (0.000001)  wd: 0.399926 (0.399924)  time: 0.468333  data: 0.245384  max mem: 5414
Epoch: [105/106]  [  60/1250]  eta: 0:10:11  loss: 2.579692 (2.565894)  lr: 0.000001 (0.000001)  wd: 0.399927 (0.399925)  time: 0.481573  data: 0.258440  max mem: 5430
Epoch: [105/106]  [  70/1250]  eta: 0:10:01  loss: 2.664371 (2.613615)  lr: 0.000001 (0.000001)  wd: 0.399928 (0.399925)  time: 0.494898  data: 0.271267  max mem: 5430
Epoch: [105/106]  [  80/1250]  eta: 0:09:47  loss: 2.568362 (2.617706)  lr: 0.000001 (0.000001)  wd: 0.399930 (0.399926)  time: 0.466341  data: 0.243274  max mem: 5430
Epoch: [105/106]  [  90/1250]  eta: 0:09:38  loss: 2.540047 (2.613104)  lr: 0.000001 (0.000001)  wd: 0.399931 (0.399927)  time: 0.461111  data: 0.238001  max mem: 5430
Epoch: [105/106]  [ 100/1250]  eta: 0:09:29  loss: 2.665023 (2.617317)  lr: 0.000001 (0.000001)  wd: 0.399932 (0.399927)  time: 0.468345  data: 0.245154  max mem: 5430
Epoch: [105/106]  [ 110/1250]  eta: 0:09:28  loss: 2.556209 (2.602507)  lr: 0.000001 (0.000001)  wd: 0.399933 (0.399928)  time: 0.497420  data: 0.274770  max mem: 5430
Epoch: [105/106]  [ 120/1250]  eta: 0:09:17  loss: 2.405098 (2.587839)  lr: 0.000001 (0.000001)  wd: 0.399934 (0.399928)  time: 0.482369  data: 0.260235  max mem: 5430
Epoch: [105/106]  [ 130/1250]  eta: 0:09:16  loss: 2.370237 (2.578387)  lr: 0.000001 (0.000001)  wd: 0.399935 (0.399929)  time: 0.486283  data: 0.263688  max mem: 5430
Epoch: [105/106]  [ 140/1250]  eta: 0:09:05  loss: 2.480066 (2.572585)  lr: 0.000001 (0.000001)  wd: 0.399937 (0.399929)  time: 0.481153  data: 0.258323  max mem: 5430
Epoch: [105/106]  [ 150/1250]  eta: 0:09:02  loss: 2.398791 (2.560254)  lr: 0.000001 (0.000001)  wd: 0.399938 (0.399930)  time: 0.468619  data: 0.245177  max mem: 5430
Epoch: [105/106]  [ 160/1250]  eta: 0:08:51  loss: 2.570218 (2.580318)  lr: 0.000001 (0.000001)  wd: 0.399939 (0.399931)  time: 0.460997  data: 0.237592  max mem: 5430
Epoch: [105/106]  [ 170/1250]  eta: 0:08:46  loss: 2.612238 (2.573518)  lr: 0.000001 (0.000001)  wd: 0.399940 (0.399931)  time: 0.448903  data: 0.226865  max mem: 5430
Epoch: [105/106]  [ 180/1250]  eta: 0:08:38  loss: 2.590219 (2.577142)  lr: 0.000001 (0.000001)  wd: 0.399941 (0.399932)  time: 0.462885  data: 0.239218  max mem: 5430
Epoch: [105/106]  [ 190/1250]  eta: 0:08:36  loss: 2.604024 (2.576744)  lr: 0.000001 (0.000001)  wd: 0.399942 (0.399932)  time: 0.486575  data: 0.262485  max mem: 5430
Epoch: [105/106]  [ 200/1250]  eta: 0:08:30  loss: 2.604024 (2.579409)  lr: 0.000001 (0.000001)  wd: 0.399943 (0.399933)  time: 0.500004  data: 0.275234  max mem: 5430
Epoch: [105/106]  [ 210/1250]  eta: 0:08:27  loss: 2.705477 (2.589502)  lr: 0.000001 (0.000001)  wd: 0.399944 (0.399933)  time: 0.485964  data: 0.260742  max mem: 5430
Epoch: [105/106]  [ 220/1250]  eta: 0:08:20  loss: 2.806975 (2.600864)  lr: 0.000001 (0.000001)  wd: 0.399945 (0.399934)  time: 0.476791  data: 0.252210  max mem: 5430
Epoch: [105/106]  [ 230/1250]  eta: 0:08:15  loss: 2.510048 (2.598384)  lr: 0.000001 (0.000001)  wd: 0.399946 (0.399935)  time: 0.467731  data: 0.244402  max mem: 5430
Epoch: [105/106]  [ 240/1250]  eta: 0:08:08  loss: 2.503933 (2.598113)  lr: 0.000001 (0.000001)  wd: 0.399947 (0.399935)  time: 0.464358  data: 0.242699  max mem: 5430
Epoch: [105/106]  [ 250/1250]  eta: 0:08:05  loss: 2.582786 (2.604763)  lr: 0.000001 (0.000001)  wd: 0.399948 (0.399936)  time: 0.475267  data: 0.254545  max mem: 5430
Epoch: [105/106]  [ 260/1250]  eta: 0:07:59  loss: 2.519491 (2.599189)  lr: 0.000001 (0.000001)  wd: 0.399949 (0.399936)  time: 0.487938  data: 0.268178  max mem: 5430
Epoch: [105/106]  [ 270/1250]  eta: 0:07:55  loss: 2.416654 (2.601150)  lr: 0.000001 (0.000001)  wd: 0.399950 (0.399937)  time: 0.490134  data: 0.270512  max mem: 5430
Epoch: [105/106]  [ 280/1250]  eta: 0:07:48  loss: 2.516683 (2.594971)  lr: 0.000001 (0.000001)  wd: 0.399951 (0.399937)  time: 0.474684  data: 0.253800  max mem: 5430
Epoch: [105/106]  [ 290/1250]  eta: 0:07:43  loss: 2.350058 (2.586145)  lr: 0.000001 (0.000001)  wd: 0.399952 (0.399938)  time: 0.451419  data: 0.228238  max mem: 5430
Epoch: [105/106]  [ 300/1250]  eta: 0:07:38  loss: 2.416763 (2.589227)  lr: 0.000001 (0.000001)  wd: 0.399953 (0.399938)  time: 0.470620  data: 0.247892  max mem: 5430
Epoch: [105/106]  [ 310/1250]  eta: 0:07:32  loss: 2.610176 (2.590408)  lr: 0.000001 (0.000001)  wd: 0.399954 (0.399939)  time: 0.449006  data: 0.227473  max mem: 5430
Epoch: [105/106]  [ 320/1250]  eta: 0:07:28  loss: 2.610176 (2.598021)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399939)  time: 0.482022  data: 0.260839  max mem: 5430
Epoch: [105/106]  [ 330/1250]  eta: 0:07:22  loss: 2.636976 (2.601121)  lr: 0.000001 (0.000001)  wd: 0.399956 (0.399940)  time: 0.489540  data: 0.268544  max mem: 5430
Epoch: [105/106]  [ 340/1250]  eta: 0:07:19  loss: 2.591720 (2.602370)  lr: 0.000001 (0.000001)  wd: 0.399957 (0.399940)  time: 0.487322  data: 0.265856  max mem: 5430
Epoch: [105/106]  [ 350/1250]  eta: 0:07:13  loss: 2.544869 (2.603561)  lr: 0.000001 (0.000001)  wd: 0.399958 (0.399941)  time: 0.483960  data: 0.262425  max mem: 5430
Epoch: [105/106]  [ 360/1250]  eta: 0:07:09  loss: 2.569692 (2.602729)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399942)  time: 0.483828  data: 0.261829  max mem: 5430
Epoch: [105/106]  [ 370/1250]  eta: 0:07:03  loss: 2.461071 (2.596768)  lr: 0.000001 (0.000001)  wd: 0.399960 (0.399942)  time: 0.479975  data: 0.258745  max mem: 5430
Epoch: [105/106]  [ 380/1250]  eta: 0:06:59  loss: 2.461071 (2.596703)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399943)  time: 0.466130  data: 0.245474  max mem: 5430
Epoch: [105/106]  [ 390/1250]  eta: 0:06:53  loss: 2.635850 (2.596960)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399943)  time: 0.459989  data: 0.237700  max mem: 5431
Epoch: [105/106]  [ 400/1250]  eta: 0:06:49  loss: 2.490229 (2.594961)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399944)  time: 0.478438  data: 0.255958  max mem: 5431
Epoch: [105/106]  [ 410/1250]  eta: 0:06:43  loss: 2.604466 (2.597764)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399944)  time: 0.469012  data: 0.248349  max mem: 5431
Epoch: [105/106]  [ 420/1250]  eta: 0:06:38  loss: 2.639560 (2.596877)  lr: 0.000001 (0.000001)  wd: 0.399964 (0.399945)  time: 0.455100  data: 0.235258  max mem: 5431
Epoch: [105/106]  [ 430/1250]  eta: 0:06:32  loss: 2.568819 (2.596611)  lr: 0.000001 (0.000001)  wd: 0.399965 (0.399945)  time: 0.458200  data: 0.237510  max mem: 5431
Epoch: [105/106]  [ 440/1250]  eta: 0:06:28  loss: 2.568819 (2.595443)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399946)  time: 0.460175  data: 0.237837  max mem: 5431
Epoch: [105/106]  [ 450/1250]  eta: 0:06:22  loss: 2.573887 (2.592998)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399946)  time: 0.470066  data: 0.248159  max mem: 5431
Epoch: [105/106]  [ 460/1250]  eta: 0:06:18  loss: 2.542247 (2.595161)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399946)  time: 0.468443  data: 0.247549  max mem: 5431
Epoch: [105/106]  [ 470/1250]  eta: 0:06:13  loss: 2.767569 (2.598256)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399947)  time: 0.475371  data: 0.254430  max mem: 5431
Epoch: [105/106]  [ 480/1250]  eta: 0:06:09  loss: 2.615667 (2.596559)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399947)  time: 0.496074  data: 0.275793  max mem: 5431
Epoch: [105/106]  [ 490/1250]  eta: 0:06:03  loss: 2.469021 (2.596814)  lr: 0.000001 (0.000001)  wd: 0.399970 (0.399948)  time: 0.493980  data: 0.273946  max mem: 5431
Epoch: [105/106]  [ 500/1250]  eta: 0:05:59  loss: 2.458673 (2.594077)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399948)  time: 0.467466  data: 0.246139  max mem: 5431
Epoch: [105/106]  [ 510/1250]  eta: 0:05:54  loss: 2.506502 (2.593261)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399949)  time: 0.465295  data: 0.243794  max mem: 5431
Epoch: [105/106]  [ 520/1250]  eta: 0:05:49  loss: 2.596817 (2.595075)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399949)  time: 0.459776  data: 0.239712  max mem: 5431
Epoch: [105/106]  [ 530/1250]  eta: 0:05:43  loss: 2.614867 (2.593491)  lr: 0.000001 (0.000001)  wd: 0.399973 (0.399950)  time: 0.451855  data: 0.232278  max mem: 5431
Epoch: [105/106]  [ 540/1250]  eta: 0:05:39  loss: 2.414848 (2.593598)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399950)  time: 0.461266  data: 0.239452  max mem: 5431
Epoch: [105/106]  [ 550/1250]  eta: 0:05:33  loss: 2.688845 (2.597080)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399951)  time: 0.464340  data: 0.241942  max mem: 5431
Epoch: [105/106]  [ 560/1250]  eta: 0:05:29  loss: 2.515218 (2.594402)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399951)  time: 0.466682  data: 0.244883  max mem: 5431
Epoch: [105/106]  [ 570/1250]  eta: 0:05:24  loss: 2.401847 (2.592474)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399952)  time: 0.473632  data: 0.251605  max mem: 5431
Epoch: [105/106]  [ 580/1250]  eta: 0:05:19  loss: 2.386482 (2.592366)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399952)  time: 0.475627  data: 0.254874  max mem: 5431
Epoch: [105/106]  [ 590/1250]  eta: 0:05:14  loss: 2.590051 (2.595863)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399952)  time: 0.471948  data: 0.251746  max mem: 5431
Epoch: [105/106]  [ 600/1250]  eta: 0:05:10  loss: 2.594786 (2.595682)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399953)  time: 0.472600  data: 0.251933  max mem: 5431
Epoch: [105/106]  [ 610/1250]  eta: 0:05:05  loss: 2.607771 (2.598093)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399953)  time: 0.481580  data: 0.259835  max mem: 5431
Epoch: [105/106]  [ 620/1250]  eta: 0:05:00  loss: 2.761765 (2.600811)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399954)  time: 0.479574  data: 0.258742  max mem: 5431
Epoch: [105/106]  [ 630/1250]  eta: 0:04:55  loss: 2.543961 (2.602097)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399954)  time: 0.453437  data: 0.233873  max mem: 5431
Epoch: [105/106]  [ 640/1250]  eta: 0:04:50  loss: 2.618140 (2.607358)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399955)  time: 0.451699  data: 0.231670  max mem: 5431
Epoch: [105/106]  [ 650/1250]  eta: 0:04:45  loss: 2.616636 (2.608014)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399955)  time: 0.455722  data: 0.235532  max mem: 5431
Epoch: [105/106]  [ 660/1250]  eta: 0:04:40  loss: 2.602599 (2.609244)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399955)  time: 0.447510  data: 0.227692  max mem: 5431
Epoch: [105/106]  [ 670/1250]  eta: 0:04:35  loss: 2.512446 (2.607251)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399956)  time: 0.469332  data: 0.249059  max mem: 5431
Epoch: [105/106]  [ 680/1250]  eta: 0:04:31  loss: 2.471652 (2.606417)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399956)  time: 0.473264  data: 0.252821  max mem: 5431
Epoch: [105/106]  [ 690/1250]  eta: 0:04:26  loss: 2.502414 (2.604816)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399957)  time: 0.472570  data: 0.252485  max mem: 5431
Epoch: [105/106]  [ 700/1250]  eta: 0:04:21  loss: 2.418086 (2.602834)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399957)  time: 0.483200  data: 0.262920  max mem: 5431
Epoch: [105/106]  [ 710/1250]  eta: 0:04:16  loss: 2.532242 (2.604618)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399957)  time: 0.485126  data: 0.264869  max mem: 5431
Epoch: [105/106]  [ 720/1250]  eta: 0:04:12  loss: 2.532242 (2.602711)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399958)  time: 0.490683  data: 0.270733  max mem: 5431
Epoch: [105/106]  [ 730/1250]  eta: 0:04:07  loss: 2.424383 (2.602008)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399958)  time: 0.479543  data: 0.258161  max mem: 5431
Epoch: [105/106]  [ 740/1250]  eta: 0:04:02  loss: 2.679924 (2.605440)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399959)  time: 0.467833  data: 0.245630  max mem: 5431
Epoch: [105/106]  [ 750/1250]  eta: 0:03:57  loss: 2.637043 (2.605592)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399959)  time: 0.475002  data: 0.252530  max mem: 5431
Epoch: [105/106]  [ 760/1250]  eta: 0:03:53  loss: 2.648358 (2.609372)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399959)  time: 0.488939  data: 0.264006  max mem: 5431
Epoch: [105/106]  [ 770/1250]  eta: 0:03:48  loss: 2.684822 (2.609390)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399960)  time: 0.482166  data: 0.257234  max mem: 5431
Epoch: [105/106]  [ 780/1250]  eta: 0:03:43  loss: 2.508925 (2.609412)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399960)  time: 0.473014  data: 0.248951  max mem: 5431
Epoch: [105/106]  [ 790/1250]  eta: 0:03:38  loss: 2.504702 (2.607397)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399960)  time: 0.478983  data: 0.255354  max mem: 5431
Epoch: [105/106]  [ 800/1250]  eta: 0:03:34  loss: 2.384208 (2.605282)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399961)  time: 0.469892  data: 0.246396  max mem: 5431
Epoch: [105/106]  [ 810/1250]  eta: 0:03:29  loss: 2.368665 (2.604065)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399961)  time: 0.447794  data: 0.223500  max mem: 5431
Epoch: [105/106]  [ 820/1250]  eta: 0:03:24  loss: 2.428991 (2.604163)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399961)  time: 0.457069  data: 0.232770  max mem: 5431
Epoch: [105/106]  [ 830/1250]  eta: 0:03:19  loss: 2.633914 (2.607179)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399962)  time: 0.467818  data: 0.244748  max mem: 5431
Epoch: [105/106]  [ 840/1250]  eta: 0:03:14  loss: 2.710523 (2.607493)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399962)  time: 0.472206  data: 0.251340  max mem: 5431
Epoch: [105/106]  [ 850/1250]  eta: 0:03:09  loss: 2.630723 (2.607662)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399963)  time: 0.471870  data: 0.252096  max mem: 5431
Epoch: [105/106]  [ 860/1250]  eta: 0:03:05  loss: 2.637905 (2.607797)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399963)  time: 0.463518  data: 0.243270  max mem: 5431
Epoch: [105/106]  [ 870/1250]  eta: 0:03:00  loss: 2.645015 (2.607572)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399963)  time: 0.466260  data: 0.245234  max mem: 5431
Epoch: [105/106]  [ 880/1250]  eta: 0:02:55  loss: 2.626439 (2.609886)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399964)  time: 0.471100  data: 0.250647  max mem: 5431
Epoch: [105/106]  [ 890/1250]  eta: 0:02:50  loss: 2.622638 (2.609568)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399964)  time: 0.468912  data: 0.248707  max mem: 5431
Epoch: [105/106]  [ 900/1250]  eta: 0:02:46  loss: 2.483762 (2.609356)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399964)  time: 0.462848  data: 0.241273  max mem: 5431
Epoch: [105/106]  [ 910/1250]  eta: 0:02:41  loss: 2.494262 (2.608878)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399965)  time: 0.465948  data: 0.244313  max mem: 5431
Epoch: [105/106]  [ 920/1250]  eta: 0:02:36  loss: 2.540777 (2.609311)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399965)  time: 0.463221  data: 0.241610  max mem: 5431
Epoch: [105/106]  [ 930/1250]  eta: 0:02:31  loss: 2.661543 (2.610005)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399965)  time: 0.480433  data: 0.258234  max mem: 5431
Epoch: [105/106]  [ 940/1250]  eta: 0:02:27  loss: 2.514042 (2.608281)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399965)  time: 0.501952  data: 0.279185  max mem: 5431
Epoch: [105/106]  [ 950/1250]  eta: 0:02:22  loss: 2.424503 (2.607334)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399966)  time: 0.460785  data: 0.238674  max mem: 5431
Epoch: [105/106]  [ 960/1250]  eta: 0:02:17  loss: 2.516110 (2.608439)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399966)  time: 0.453810  data: 0.233404  max mem: 5431
Epoch: [105/106]  [ 970/1250]  eta: 0:02:12  loss: 2.573934 (2.608138)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399966)  time: 0.488112  data: 0.268346  max mem: 5431
Epoch: [105/106]  [ 980/1250]  eta: 0:02:08  loss: 2.544676 (2.609730)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399967)  time: 0.495073  data: 0.273957  max mem: 5431
Epoch: [105/106]  [ 990/1250]  eta: 0:02:03  loss: 2.544676 (2.609008)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399967)  time: 0.492613  data: 0.270757  max mem: 5431
Epoch: [105/106]  [1000/1250]  eta: 0:01:58  loss: 2.483781 (2.609011)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399967)  time: 0.490461  data: 0.269894  max mem: 5431
Epoch: [105/106]  [1010/1250]  eta: 0:01:54  loss: 2.501262 (2.608348)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.494592  data: 0.273311  max mem: 5431
Epoch: [105/106]  [1020/1250]  eta: 0:01:49  loss: 2.523538 (2.609601)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.504578  data: 0.282684  max mem: 5431
Epoch: [105/106]  [1030/1250]  eta: 0:01:44  loss: 2.468656 (2.607917)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.483781  data: 0.262701  max mem: 5431
Epoch: [105/106]  [1040/1250]  eta: 0:01:39  loss: 2.470698 (2.608354)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399968)  time: 0.462438  data: 0.241694  max mem: 5431
Epoch: [105/106]  [1050/1250]  eta: 0:01:35  loss: 2.616874 (2.609692)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399969)  time: 0.477161  data: 0.255772  max mem: 5431
Epoch: [105/106]  [1060/1250]  eta: 0:01:30  loss: 2.522275 (2.610197)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399969)  time: 0.479872  data: 0.258026  max mem: 5431
Epoch: [105/106]  [1070/1250]  eta: 0:01:25  loss: 2.568197 (2.613361)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399969)  time: 0.485529  data: 0.264360  max mem: 5431
Epoch: [105/106]  [1080/1250]  eta: 0:01:20  loss: 2.632385 (2.613796)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399970)  time: 0.451430  data: 0.231152  max mem: 5431
Epoch: [105/106]  [1090/1250]  eta: 0:01:16  loss: 2.583275 (2.613769)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.449320  data: 0.228394  max mem: 5431
Epoch: [105/106]  [1100/1250]  eta: 0:01:11  loss: 2.519052 (2.613980)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.463138  data: 0.242556  max mem: 5431
Epoch: [105/106]  [1110/1250]  eta: 0:01:06  loss: 2.498322 (2.613314)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.474953  data: 0.255015  max mem: 5431
Epoch: [105/106]  [1120/1250]  eta: 0:01:01  loss: 2.554017 (2.612717)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.483963  data: 0.263084  max mem: 5431
Epoch: [105/106]  [1130/1250]  eta: 0:00:56  loss: 2.613276 (2.611662)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.464922  data: 0.244340  max mem: 5431
Epoch: [105/106]  [1140/1250]  eta: 0:00:52  loss: 2.475178 (2.610429)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.465042  data: 0.244534  max mem: 5431
Epoch: [105/106]  [1150/1250]  eta: 0:00:47  loss: 2.421895 (2.611510)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.455033  data: 0.234615  max mem: 5431
Epoch: [105/106]  [1160/1250]  eta: 0:00:42  loss: 2.515025 (2.611946)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399972)  time: 0.464898  data: 0.245254  max mem: 5431
Epoch: [105/106]  [1170/1250]  eta: 0:00:37  loss: 2.515025 (2.612730)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.480520  data: 0.260293  max mem: 5431
Epoch: [105/106]  [1180/1250]  eta: 0:00:33  loss: 2.655886 (2.613456)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.466225  data: 0.245803  max mem: 5431
Epoch: [105/106]  [1190/1250]  eta: 0:00:28  loss: 2.643459 (2.614497)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.470643  data: 0.251035  max mem: 5431
Epoch: [105/106]  [1200/1250]  eta: 0:00:23  loss: 2.531798 (2.614900)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.470573  data: 0.250317  max mem: 5431
Epoch: [105/106]  [1210/1250]  eta: 0:00:18  loss: 2.476997 (2.614381)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.460413  data: 0.239518  max mem: 5431
Epoch: [105/106]  [1220/1250]  eta: 0:00:14  loss: 2.535342 (2.614336)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.490617  data: 0.269917  max mem: 5431
Epoch: [105/106]  [1230/1250]  eta: 0:00:09  loss: 2.492251 (2.613296)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.490205  data: 0.269390  max mem: 5431
Epoch: [105/106]  [1240/1250]  eta: 0:00:04  loss: 2.408652 (2.612735)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.470202  data: 0.250114  max mem: 5431
Epoch: [105/106]  [1249/1250]  eta: 0:00:00  loss: 2.494450 (2.612837)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)  time: 0.475352  data: 0.256063  max mem: 5431
Epoch: [105/106] Total time: 0:09:53 (0.475018 s / it)
Averaged stats: loss: 2.494450 (2.612837)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)
Training time 0:09:54
Wrote profile results to main_dino.py.lprof
Timer unit: 1e-06 s

Total time: 593.456 s
File: main_dino.py
Function: train_one_epoch at line 490

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   490                                           @profile
   491                                           def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,
   492                                                               optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,
   493                                                               fp16_scaler, args):
   494         1         23.7     23.7      0.0      metric_logger = utils.MetricLogger(delimiter="  ")
   495         1          6.3      6.3      0.0      header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)
   496      1250  315497842.6 252398.3     53.2      for it, (images, corrs, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):
   497                                                   # show_batch_images(data, args.batch_size_per_gpu)
   498                                           
   499                                                   # update weight decay and learning rate according to their schedule
   500      1250      13458.0     10.8      0.0          it = len(data_loader) * epoch + it  # global training iteration
   501      2500       3357.9      1.3      0.0          for i, param_group in enumerate(optimizer.param_groups):
   502      2500       4800.3      1.9      0.0              param_group["lr"] = lr_schedule[it]
   503      1250        736.6      0.6      0.0              if i == 0:  # only the first group is regularized
   504      1250       1579.8      1.3      0.0                  param_group["weight_decay"] = wd_schedule[it]
   505                                           
   506                                                   # # Decompose data:
   507                                                   # images =[
   508                                                   #     torch.empty((len(data),3,args.global_scale,args.global_scale)),torch.empty((len(data),3,args.global_scale,args.global_scale)),
   509                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   510                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   511                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   512                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   513                                                   # ]
   514                                                   # for i in range(len(data[0])):
   515                                                   #     for j in range(len(data)):
   516                                                   #         images[i][j] = data[j][i].crop_tensor_normed.detach().clone()
   517                                           
   518                                                   # move images to gpu
   519      1250     262448.8    210.0      0.0          images = [im.cuda(non_blocking=True) for im in images]
   520                                                   # teacher and student forward passes + compute dino loss
   521      1250      54720.7     43.8      0.0          with torch.cuda.amp.autocast(fp16_scaler is not None):
   522      1250   18262186.0  14609.7      3.1              teacher_output = teacher(images[:2], args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)  # only the 2 global views pass through the teacher
   523      1250   67944803.2  54355.8     11.4              student_output = student(images, args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)
   524      1250   22587608.6  18070.1      3.8              loss = dino_loss(student_output, teacher_output, corrs, epoch)
   525                                           
   526      1250      42269.2     33.8      0.0          if not math.isfinite(loss.item()):
   527                                                       print("Loss is {}, stopping training".format(loss.item()), force=True)
   528                                                       sys.exit(1)
   529                                           
   530                                                   # student update
   531      1250    1529728.6   1223.8      0.3          optimizer.zero_grad()
   532      1250       7458.2      6.0      0.0          param_norms = None
   533      1250        554.3      0.4      0.0          if fp16_scaler is None:
   534                                                       loss.backward()
   535                                                       if args.clip_grad:
   536                                                           param_norms = utils.clip_gradients(student, args.clip_grad)
   537                                                       utils.cancel_gradients_last_layer(epoch, student,
   538                                                                                         args.freeze_last_layer)
   539                                                       optimizer.step()
   540                                                   else:
   541      1250   87297463.7  69838.0     14.7              fp16_scaler.scale(loss).backward()
   542      1250       4152.2      3.3      0.0              if args.clip_grad:
   543      1250    1901042.5   1520.8      0.3                  fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
   544      1250   51944028.9  41555.2      8.8                  param_norms = utils.clip_gradients(student, args.clip_grad)
   545      1250       2870.8      2.3      0.0              utils.cancel_gradients_last_layer(epoch, student,
   546      1250       2213.3      1.8      0.0                                                args.freeze_last_layer)
   547      1250   17198926.7  13759.1      2.9              fp16_scaler.step(optimizer)
   548      1250      52143.8     41.7      0.0              fp16_scaler.update()
   549                                           
   550                                                   # EMA update for the teacher
   551      1250       9881.9      7.9      0.0          with torch.no_grad():
   552      1250       2622.4      2.1      0.0              m = momentum_schedule[it]  # momentum parameter
   553    197500    2592491.3     13.1      0.4              for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):
   554    197500    6089665.1     30.8      1.0                  param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
   555                                           
   556                                                   # logging
   557      1250      84637.6     67.7      0.0          torch.cuda.synchronize()
   558      1250      45953.5     36.8      0.0          metric_logger.update(loss=loss.item())
   559      1250       7517.2      6.0      0.0          metric_logger.update(lr=optimizer.param_groups[0]["lr"])
   560      1250       4877.7      3.9      0.0          metric_logger.update(wd=optimizer.param_groups[0]["weight_decay"])
   561                                               # gather the stats from all processes
   562         1       1250.3   1250.3      0.0      metric_logger.synchronize_between_processes()
   563         1        437.2    437.2      0.0      print("Averaged stats:", metric_logger)
   564         1         16.7     16.7      0.0      return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: 28fcf3dd2d6a975b9f9ca5b35a0244e7ec6ad620, status: has uncommited changes, branch: QCRI_Cifar10_same_batch_augmentation_correspondence_on_CPU

arch: vit_tiny
batch_size_per_gpu: 40
clip_grad: 3.0
data_path: /home/alij/Datasets/Cifar10/train
dist_url: env://
drop_path_rate: 0.1
epochs: 107
freeze_last_layer: 1
global_crops_number: 2
global_crops_scale: (0.4, 1.0)
global_scale: 224
gpu: 0
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
local_scale: 96
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
norm_last_layer: True
num_workers: 4
optimizer: adamw
out_dim: 1000
output_dir: /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu
patch_size: 16
rank: 0
saveckp_freq: 20
seed: 0
teacher_temp: 0.04
use_bn_in_head: False
use_fp16: True
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 50000 images.
Student and Teacher are built: they are both vit_tiny network.
Loss, optimizer and schedulers ready.
Found checkpoint at /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth
=> loaded 'student' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'fp16_scaler' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
Starting DINO training !
Epoch: [106/107]  [   0/1250]  eta: 1:31:57  loss: 2.754765 (2.754765)  lr: 0.000001 (0.000001)  wd: 0.399922 (0.399922)  time: 4.413800  data: 1.858447  max mem: 5312
Epoch: [106/107]  [  10/1250]  eta: 0:12:35  loss: 2.627098 (2.588044)  lr: 0.000001 (0.000001)  wd: 0.399923 (0.399923)  time: 0.609452  data: 0.171049  max mem: 5407
Epoch: [106/107]  [  20/1250]  eta: 0:10:09  loss: 2.561178 (2.649338)  lr: 0.000001 (0.000001)  wd: 0.399924 (0.399924)  time: 0.299714  data: 0.075138  max mem: 5409
Epoch: [106/107]  [  30/1250]  eta: 0:09:36  loss: 2.559361 (2.622369)  lr: 0.000001 (0.000001)  wd: 0.399925 (0.399924)  time: 0.396808  data: 0.172972  max mem: 5414
Epoch: [106/107]  [  40/1250]  eta: 0:08:58  loss: 2.559361 (2.637094)  lr: 0.000001 (0.000001)  wd: 0.399926 (0.399925)  time: 0.392237  data: 0.168409  max mem: 5414
Epoch: [106/107]  [  50/1250]  eta: 0:08:42  loss: 2.539587 (2.603117)  lr: 0.000001 (0.000001)  wd: 0.399927 (0.399925)  time: 0.377731  data: 0.155946  max mem: 5414
Epoch: [106/107]  [  60/1250]  eta: 0:08:28  loss: 2.512110 (2.599189)  lr: 0.000001 (0.000001)  wd: 0.399929 (0.399926)  time: 0.391805  data: 0.167477  max mem: 5430
Epoch: [106/107]  [  70/1250]  eta: 0:08:25  loss: 2.579931 (2.627420)  lr: 0.000001 (0.000001)  wd: 0.399930 (0.399927)  time: 0.409726  data: 0.184849  max mem: 5430
Epoch: [106/107]  [  80/1250]  eta: 0:08:13  loss: 2.635385 (2.625762)  lr: 0.000001 (0.000001)  wd: 0.399931 (0.399927)  time: 0.403277  data: 0.181231  max mem: 5430
Epoch: [106/107]  [  90/1250]  eta: 0:08:10  loss: 2.802306 (2.641967)  lr: 0.000001 (0.000001)  wd: 0.399932 (0.399928)  time: 0.405665  data: 0.183692  max mem: 5430
Epoch: [106/107]  [ 100/1250]  eta: 0:07:59  loss: 2.721780 (2.647368)  lr: 0.000001 (0.000001)  wd: 0.399933 (0.399928)  time: 0.399955  data: 0.178007  max mem: 5430
Epoch: [106/107]  [ 110/1250]  eta: 0:08:01  loss: 2.487503 (2.627163)  lr: 0.000001 (0.000001)  wd: 0.399934 (0.399929)  time: 0.417860  data: 0.193864  max mem: 5430
Epoch: [106/107]  [ 120/1250]  eta: 0:07:48  loss: 2.434150 (2.618684)  lr: 0.000001 (0.000001)  wd: 0.399935 (0.399930)  time: 0.401259  data: 0.176228  max mem: 5430
Epoch: [106/107]  [ 130/1250]  eta: 0:07:47  loss: 2.379029 (2.596124)  lr: 0.000001 (0.000001)  wd: 0.399937 (0.399930)  time: 0.388453  data: 0.164571  max mem: 5430
Epoch: [106/107]  [ 140/1250]  eta: 0:07:41  loss: 2.410168 (2.589066)  lr: 0.000001 (0.000001)  wd: 0.399938 (0.399931)  time: 0.423438  data: 0.197556  max mem: 5430
Epoch: [106/107]  [ 150/1250]  eta: 0:07:38  loss: 2.423511 (2.578793)  lr: 0.000001 (0.000001)  wd: 0.399939 (0.399931)  time: 0.412341  data: 0.186057  max mem: 5430
Epoch: [106/107]  [ 160/1250]  eta: 0:07:28  loss: 2.500095 (2.599165)  lr: 0.000001 (0.000001)  wd: 0.399940 (0.399932)  time: 0.382530  data: 0.158850  max mem: 5430
Epoch: [106/107]  [ 170/1250]  eta: 0:07:26  loss: 2.570041 (2.596148)  lr: 0.000001 (0.000001)  wd: 0.399941 (0.399932)  time: 0.391174  data: 0.167749  max mem: 5430
Epoch: [106/107]  [ 180/1250]  eta: 0:07:18  loss: 2.457617 (2.591799)  lr: 0.000001 (0.000001)  wd: 0.399942 (0.399933)  time: 0.395714  data: 0.172459  max mem: 5430
Epoch: [106/107]  [ 190/1250]  eta: 0:07:15  loss: 2.526324 (2.593960)  lr: 0.000001 (0.000001)  wd: 0.399943 (0.399934)  time: 0.389038  data: 0.162766  max mem: 5430
Epoch: [106/107]  [ 200/1250]  eta: 0:07:07  loss: 2.580350 (2.597259)  lr: 0.000001 (0.000001)  wd: 0.399944 (0.399934)  time: 0.385142  data: 0.157408  max mem: 5430
Epoch: [106/107]  [ 210/1250]  eta: 0:07:04  loss: 2.584230 (2.601359)  lr: 0.000001 (0.000001)  wd: 0.399945 (0.399935)  time: 0.380732  data: 0.151610  max mem: 5430
Epoch: [106/107]  [ 220/1250]  eta: 0:06:55  loss: 2.752969 (2.613900)  lr: 0.000001 (0.000001)  wd: 0.399946 (0.399935)  time: 0.361786  data: 0.125316  max mem: 5430
Epoch: [106/107]  [ 230/1250]  eta: 0:06:51  loss: 2.678250 (2.613012)  lr: 0.000001 (0.000001)  wd: 0.399947 (0.399936)  time: 0.353329  data: 0.110032  max mem: 5430
Epoch: [106/107]  [ 240/1250]  eta: 0:06:44  loss: 2.572628 (2.614494)  lr: 0.000001 (0.000001)  wd: 0.399948 (0.399936)  time: 0.372738  data: 0.137346  max mem: 5430
Epoch: [106/107]  [ 250/1250]  eta: 0:06:40  loss: 2.654539 (2.621803)  lr: 0.000001 (0.000001)  wd: 0.399949 (0.399937)  time: 0.373036  data: 0.148283  max mem: 5430
Epoch: [106/107]  [ 260/1250]  eta: 0:06:36  loss: 2.609799 (2.620928)  lr: 0.000001 (0.000001)  wd: 0.399950 (0.399937)  time: 0.394752  data: 0.161454  max mem: 5430
Epoch: [106/107]  [ 270/1250]  eta: 0:06:31  loss: 2.514357 (2.620258)  lr: 0.000001 (0.000001)  wd: 0.399951 (0.399938)  time: 0.376558  data: 0.134103  max mem: 5430
Epoch: [106/107]  [ 280/1250]  eta: 0:06:26  loss: 2.450384 (2.615493)  lr: 0.000001 (0.000001)  wd: 0.399952 (0.399938)  time: 0.376039  data: 0.133917  max mem: 5430
Epoch: [106/107]  [ 290/1250]  eta: 0:06:21  loss: 2.403913 (2.609373)  lr: 0.000001 (0.000001)  wd: 0.399953 (0.399939)  time: 0.380861  data: 0.147872  max mem: 5430
Epoch: [106/107]  [ 300/1250]  eta: 0:06:17  loss: 2.403913 (2.609102)  lr: 0.000001 (0.000001)  wd: 0.399954 (0.399940)  time: 0.380304  data: 0.151134  max mem: 5430
Epoch: [106/107]  [ 310/1250]  eta: 0:06:12  loss: 2.409542 (2.606494)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399940)  time: 0.378873  data: 0.148209  max mem: 5430
Epoch: [106/107]  [ 320/1250]  eta: 0:06:08  loss: 2.483409 (2.610884)  lr: 0.000001 (0.000001)  wd: 0.399956 (0.399941)  time: 0.385062  data: 0.155144  max mem: 5430
Epoch: [106/107]  [ 330/1250]  eta: 0:06:03  loss: 2.578467 (2.612240)  lr: 0.000001 (0.000001)  wd: 0.399957 (0.399941)  time: 0.379005  data: 0.148814  max mem: 5430
Epoch: [106/107]  [ 340/1250]  eta: 0:06:00  loss: 2.604837 (2.613188)  lr: 0.000001 (0.000001)  wd: 0.399958 (0.399942)  time: 0.389063  data: 0.162796  max mem: 5430
Epoch: [106/107]  [ 350/1250]  eta: 0:05:55  loss: 2.604837 (2.610522)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399942)  time: 0.383738  data: 0.160026  max mem: 5430
Epoch: [106/107]  [ 360/1250]  eta: 0:05:51  loss: 2.597682 (2.610798)  lr: 0.000001 (0.000001)  wd: 0.399960 (0.399943)  time: 0.382807  data: 0.158451  max mem: 5430
Epoch: [106/107]  [ 370/1250]  eta: 0:05:46  loss: 2.438107 (2.604646)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399943)  time: 0.373886  data: 0.149451  max mem: 5430
Epoch: [106/107]  [ 380/1250]  eta: 0:05:43  loss: 2.382771 (2.603466)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399944)  time: 0.373884  data: 0.151185  max mem: 5430
Epoch: [106/107]  [ 390/1250]  eta: 0:05:38  loss: 2.503424 (2.602334)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399944)  time: 0.383242  data: 0.160725  max mem: 5431
Epoch: [106/107]  [ 400/1250]  eta: 0:05:34  loss: 2.459176 (2.599409)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399945)  time: 0.372850  data: 0.149579  max mem: 5431
Epoch: [106/107]  [ 410/1250]  eta: 0:05:29  loss: 2.494454 (2.599760)  lr: 0.000001 (0.000001)  wd: 0.399964 (0.399945)  time: 0.378423  data: 0.154179  max mem: 5431
Epoch: [106/107]  [ 420/1250]  eta: 0:05:25  loss: 2.649994 (2.602167)  lr: 0.000001 (0.000001)  wd: 0.399965 (0.399946)  time: 0.374581  data: 0.149685  max mem: 5431
Epoch: [106/107]  [ 430/1250]  eta: 0:05:20  loss: 2.676025 (2.605898)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399946)  time: 0.360578  data: 0.137194  max mem: 5431
Epoch: [106/107]  [ 440/1250]  eta: 0:05:16  loss: 2.676345 (2.606286)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399947)  time: 0.357012  data: 0.134863  max mem: 5431
Epoch: [106/107]  [ 450/1250]  eta: 0:05:11  loss: 2.476319 (2.605361)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399947)  time: 0.358831  data: 0.137691  max mem: 5431
Epoch: [106/107]  [ 460/1250]  eta: 0:05:08  loss: 2.476319 (2.604812)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399947)  time: 0.381875  data: 0.160457  max mem: 5431
Epoch: [106/107]  [ 470/1250]  eta: 0:05:04  loss: 2.623121 (2.605680)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399948)  time: 0.394984  data: 0.172208  max mem: 5431
Epoch: [106/107]  [ 480/1250]  eta: 0:05:00  loss: 2.646522 (2.603649)  lr: 0.000001 (0.000001)  wd: 0.399970 (0.399948)  time: 0.400070  data: 0.176555  max mem: 5431
Epoch: [106/107]  [ 490/1250]  eta: 0:04:56  loss: 2.587726 (2.602391)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399949)  time: 0.399331  data: 0.176383  max mem: 5431
Epoch: [106/107]  [ 500/1250]  eta: 0:04:52  loss: 2.587726 (2.602146)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399949)  time: 0.370939  data: 0.148009  max mem: 5431
Epoch: [106/107]  [ 510/1250]  eta: 0:04:48  loss: 2.618161 (2.601504)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399950)  time: 0.366384  data: 0.144187  max mem: 5431
Epoch: [106/107]  [ 520/1250]  eta: 0:04:43  loss: 2.446447 (2.600006)  lr: 0.000001 (0.000001)  wd: 0.399973 (0.399950)  time: 0.362171  data: 0.139993  max mem: 5431
Epoch: [106/107]  [ 530/1250]  eta: 0:04:39  loss: 2.466276 (2.601138)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399951)  time: 0.368990  data: 0.145969  max mem: 5431
Epoch: [106/107]  [ 540/1250]  eta: 0:04:35  loss: 2.593579 (2.601799)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399951)  time: 0.372669  data: 0.151204  max mem: 5431
Epoch: [106/107]  [ 550/1250]  eta: 0:04:31  loss: 2.696411 (2.604280)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399952)  time: 0.373606  data: 0.152647  max mem: 5431
Epoch: [106/107]  [ 560/1250]  eta: 0:04:27  loss: 2.642873 (2.601553)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399952)  time: 0.377651  data: 0.156341  max mem: 5431
Epoch: [106/107]  [ 570/1250]  eta: 0:04:23  loss: 2.405396 (2.598315)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399952)  time: 0.378344  data: 0.156867  max mem: 5431
Epoch: [106/107]  [ 580/1250]  eta: 0:04:19  loss: 2.316066 (2.596882)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399953)  time: 0.383786  data: 0.160919  max mem: 5431
Epoch: [106/107]  [ 590/1250]  eta: 0:04:15  loss: 2.316066 (2.597612)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399953)  time: 0.374060  data: 0.150266  max mem: 5431
Epoch: [106/107]  [ 600/1250]  eta: 0:04:11  loss: 2.532279 (2.596694)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399954)  time: 0.381165  data: 0.157594  max mem: 5431
Epoch: [106/107]  [ 610/1250]  eta: 0:04:07  loss: 2.685945 (2.599104)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399954)  time: 0.391320  data: 0.168195  max mem: 5431
Epoch: [106/107]  [ 620/1250]  eta: 0:04:04  loss: 2.769478 (2.601848)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399955)  time: 0.386078  data: 0.164365  max mem: 5431
Epoch: [106/107]  [ 630/1250]  eta: 0:03:59  loss: 2.616843 (2.603892)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399955)  time: 0.373716  data: 0.152681  max mem: 5431
Epoch: [106/107]  [ 640/1250]  eta: 0:03:55  loss: 2.724395 (2.605939)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399955)  time: 0.353996  data: 0.132442  max mem: 5431
Epoch: [106/107]  [ 650/1250]  eta: 0:03:52  loss: 2.589026 (2.604920)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399956)  time: 0.397895  data: 0.174649  max mem: 5431
Epoch: [106/107]  [ 660/1250]  eta: 0:03:48  loss: 2.510571 (2.607051)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399956)  time: 0.392363  data: 0.169330  max mem: 5431
Epoch: [106/107]  [ 670/1250]  eta: 0:03:44  loss: 2.563537 (2.605072)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399957)  time: 0.388833  data: 0.165757  max mem: 5431
Epoch: [106/107]  [ 680/1250]  eta: 0:03:40  loss: 2.486707 (2.602186)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399957)  time: 0.391634  data: 0.165852  max mem: 5431
Epoch: [106/107]  [ 690/1250]  eta: 0:03:36  loss: 2.487167 (2.601471)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399957)  time: 0.379250  data: 0.153793  max mem: 5431
Epoch: [106/107]  [ 700/1250]  eta: 0:03:32  loss: 2.487167 (2.599183)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399958)  time: 0.372606  data: 0.148869  max mem: 5431
Epoch: [106/107]  [ 710/1250]  eta: 0:03:28  loss: 2.596064 (2.601298)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399958)  time: 0.375797  data: 0.152405  max mem: 5431
Epoch: [106/107]  [ 720/1250]  eta: 0:03:24  loss: 2.654751 (2.600325)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399959)  time: 0.386940  data: 0.164790  max mem: 5431
Epoch: [106/107]  [ 730/1250]  eta: 0:03:20  loss: 2.491385 (2.601032)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399959)  time: 0.353914  data: 0.132408  max mem: 5431
Epoch: [106/107]  [ 740/1250]  eta: 0:03:17  loss: 2.629500 (2.604934)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399959)  time: 0.383908  data: 0.161552  max mem: 5431
Epoch: [106/107]  [ 750/1250]  eta: 0:03:13  loss: 2.794484 (2.605627)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399960)  time: 0.393749  data: 0.171683  max mem: 5431
Epoch: [106/107]  [ 760/1250]  eta: 0:03:09  loss: 2.712869 (2.608694)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399960)  time: 0.392060  data: 0.170178  max mem: 5431
Epoch: [106/107]  [ 770/1250]  eta: 0:03:05  loss: 2.649214 (2.610448)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399960)  time: 0.384860  data: 0.162781  max mem: 5431
Epoch: [106/107]  [ 780/1250]  eta: 0:03:01  loss: 2.558861 (2.611204)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399961)  time: 0.376286  data: 0.153620  max mem: 5431
Epoch: [106/107]  [ 790/1250]  eta: 0:02:57  loss: 2.513549 (2.609259)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399961)  time: 0.386259  data: 0.163635  max mem: 5431
Epoch: [106/107]  [ 800/1250]  eta: 0:02:53  loss: 2.433934 (2.607968)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399961)  time: 0.379589  data: 0.156612  max mem: 5431
Epoch: [106/107]  [ 810/1250]  eta: 0:02:49  loss: 2.395558 (2.607753)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399962)  time: 0.354615  data: 0.131745  max mem: 5431
Epoch: [106/107]  [ 820/1250]  eta: 0:02:45  loss: 2.408655 (2.606839)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399962)  time: 0.356646  data: 0.134290  max mem: 5431
Epoch: [106/107]  [ 830/1250]  eta: 0:02:41  loss: 2.720910 (2.610052)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399963)  time: 0.373351  data: 0.151525  max mem: 5431
Epoch: [106/107]  [ 840/1250]  eta: 0:02:37  loss: 2.720910 (2.610553)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399963)  time: 0.381889  data: 0.160608  max mem: 5431
Epoch: [106/107]  [ 850/1250]  eta: 0:02:33  loss: 2.621083 (2.610928)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399963)  time: 0.376472  data: 0.154200  max mem: 5431
Epoch: [106/107]  [ 860/1250]  eta: 0:02:30  loss: 2.562440 (2.610102)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399964)  time: 0.367282  data: 0.145329  max mem: 5431
Epoch: [106/107]  [ 870/1250]  eta: 0:02:26  loss: 2.557848 (2.608883)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399964)  time: 0.367106  data: 0.144466  max mem: 5431
Epoch: [106/107]  [ 880/1250]  eta: 0:02:22  loss: 2.582574 (2.611212)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399964)  time: 0.377711  data: 0.152620  max mem: 5431
Epoch: [106/107]  [ 890/1250]  eta: 0:02:18  loss: 2.606557 (2.610650)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399965)  time: 0.377479  data: 0.154008  max mem: 5431
Epoch: [106/107]  [ 900/1250]  eta: 0:02:14  loss: 2.484629 (2.609769)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399965)  time: 0.366308  data: 0.144243  max mem: 5431
Epoch: [106/107]  [ 910/1250]  eta: 0:02:10  loss: 2.429274 (2.609219)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399965)  time: 0.367105  data: 0.145029  max mem: 5431
Epoch: [106/107]  [ 920/1250]  eta: 0:02:06  loss: 2.509556 (2.610553)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399966)  time: 0.365996  data: 0.144303  max mem: 5431
Epoch: [106/107]  [ 930/1250]  eta: 0:02:02  loss: 2.527944 (2.610714)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399966)  time: 0.387072  data: 0.165193  max mem: 5431
Epoch: [106/107]  [ 940/1250]  eta: 0:01:59  loss: 2.480119 (2.608496)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399966)  time: 0.412491  data: 0.189706  max mem: 5431
Epoch: [106/107]  [ 950/1250]  eta: 0:01:55  loss: 2.373493 (2.607314)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399966)  time: 0.384208  data: 0.161305  max mem: 5431
Epoch: [106/107]  [ 960/1250]  eta: 0:01:51  loss: 2.466966 (2.608375)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399967)  time: 0.390771  data: 0.169299  max mem: 5431
Epoch: [106/107]  [ 970/1250]  eta: 0:01:47  loss: 2.501582 (2.607765)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399967)  time: 0.422009  data: 0.200208  max mem: 5431
Epoch: [106/107]  [ 980/1250]  eta: 0:01:43  loss: 2.465314 (2.609001)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399967)  time: 0.411117  data: 0.188107  max mem: 5431
Epoch: [106/107]  [ 990/1250]  eta: 0:01:40  loss: 2.579967 (2.609986)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399968)  time: 0.396941  data: 0.174449  max mem: 5431
Epoch: [106/107]  [1000/1250]  eta: 0:01:36  loss: 2.505410 (2.608594)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.397792  data: 0.176244  max mem: 5431
Epoch: [106/107]  [1010/1250]  eta: 0:01:32  loss: 2.437330 (2.607242)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.398451  data: 0.176154  max mem: 5431
Epoch: [106/107]  [1020/1250]  eta: 0:01:28  loss: 2.554955 (2.608994)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399968)  time: 0.410476  data: 0.186848  max mem: 5431
Epoch: [106/107]  [1030/1250]  eta: 0:01:24  loss: 2.400899 (2.606905)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399969)  time: 0.397514  data: 0.173103  max mem: 5431
Epoch: [106/107]  [1040/1250]  eta: 0:01:20  loss: 2.400899 (2.607326)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399969)  time: 0.369625  data: 0.146682  max mem: 5431
Epoch: [106/107]  [1050/1250]  eta: 0:01:17  loss: 2.772085 (2.609329)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399969)  time: 0.380416  data: 0.159051  max mem: 5431
Epoch: [106/107]  [1060/1250]  eta: 0:01:13  loss: 2.772085 (2.610321)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399970)  time: 0.384470  data: 0.162389  max mem: 5431
Epoch: [106/107]  [1070/1250]  eta: 0:01:09  loss: 2.659777 (2.613541)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399970)  time: 0.391576  data: 0.169716  max mem: 5431
Epoch: [106/107]  [1080/1250]  eta: 0:01:05  loss: 2.708789 (2.614506)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399970)  time: 0.359490  data: 0.138064  max mem: 5431
Epoch: [106/107]  [1090/1250]  eta: 0:01:01  loss: 2.632222 (2.614163)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399970)  time: 0.361069  data: 0.138111  max mem: 5431
Epoch: [106/107]  [1100/1250]  eta: 0:00:57  loss: 2.516648 (2.615656)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.368047  data: 0.145899  max mem: 5431
Epoch: [106/107]  [1110/1250]  eta: 0:00:53  loss: 2.516648 (2.615245)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.375401  data: 0.154028  max mem: 5431
Epoch: [106/107]  [1120/1250]  eta: 0:00:50  loss: 2.581741 (2.614388)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.395748  data: 0.173481  max mem: 5431
Epoch: [106/107]  [1130/1250]  eta: 0:00:46  loss: 2.581741 (2.613730)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399971)  time: 0.379130  data: 0.155837  max mem: 5431
Epoch: [106/107]  [1140/1250]  eta: 0:00:42  loss: 2.515273 (2.612599)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399972)  time: 0.366675  data: 0.143640  max mem: 5431
Epoch: [106/107]  [1150/1250]  eta: 0:00:38  loss: 2.431038 (2.612855)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399972)  time: 0.384129  data: 0.161374  max mem: 5431
Epoch: [106/107]  [1160/1250]  eta: 0:00:34  loss: 2.510891 (2.613975)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.367781  data: 0.145608  max mem: 5431
Epoch: [106/107]  [1170/1250]  eta: 0:00:30  loss: 2.675150 (2.614644)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.379944  data: 0.157462  max mem: 5431
Epoch: [106/107]  [1180/1250]  eta: 0:00:26  loss: 2.492788 (2.614157)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.384760  data: 0.162263  max mem: 5431
Epoch: [106/107]  [1190/1250]  eta: 0:00:23  loss: 2.635452 (2.615796)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.398321  data: 0.177132  max mem: 5431
Epoch: [106/107]  [1200/1250]  eta: 0:00:19  loss: 2.719354 (2.617061)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.404860  data: 0.183737  max mem: 5431
Epoch: [106/107]  [1210/1250]  eta: 0:00:15  loss: 2.548286 (2.615976)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.378642  data: 0.157430  max mem: 5431
Epoch: [106/107]  [1220/1250]  eta: 0:00:11  loss: 2.475311 (2.615272)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399973)  time: 0.376574  data: 0.154755  max mem: 5431
Epoch: [106/107]  [1230/1250]  eta: 0:00:07  loss: 2.475311 (2.614212)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)  time: 0.379202  data: 0.156357  max mem: 5431
Epoch: [106/107]  [1240/1250]  eta: 0:00:03  loss: 2.491561 (2.613638)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)  time: 0.378245  data: 0.155322  max mem: 5431
Epoch: [106/107]  [1249/1250]  eta: 0:00:00  loss: 2.513236 (2.613692)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)  time: 0.353683  data: 0.131723  max mem: 5431
Epoch: [106/107] Total time: 0:08:00 (0.384439 s / it)
Averaged stats: loss: 2.513236 (2.613692)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399974)
Training time 0:08:00
Wrote profile results to main_dino.py.lprof
Timer unit: 1e-06 s

Total time: 480.233 s
File: main_dino.py
Function: train_one_epoch at line 490

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   490                                           @profile
   491                                           def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,
   492                                                               optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,
   493                                                               fp16_scaler, args):
   494         1         16.3     16.3      0.0      metric_logger = utils.MetricLogger(delimiter="  ")
   495         1          3.9      3.9      0.0      header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)
   496      1250  198972337.9 159177.9     41.4      for it, (images, corrs, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):
   497                                                   # show_batch_images(data, args.batch_size_per_gpu)
   498                                           
   499                                                   # update weight decay and learning rate according to their schedule
   500      1250      17877.5     14.3      0.0          it = len(data_loader) * epoch + it  # global training iteration
   501      2500       4048.1      1.6      0.0          for i, param_group in enumerate(optimizer.param_groups):
   502      2500       5219.0      2.1      0.0              param_group["lr"] = lr_schedule[it]
   503      1250        660.8      0.5      0.0              if i == 0:  # only the first group is regularized
   504      1250       1577.8      1.3      0.0                  param_group["weight_decay"] = wd_schedule[it]
   505                                           
   506                                                   # # Decompose data:
   507                                                   # images =[
   508                                                   #     torch.empty((len(data),3,args.global_scale,args.global_scale)),torch.empty((len(data),3,args.global_scale,args.global_scale)),
   509                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   510                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   511                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   512                                                   #     torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   513                                                   # ]
   514                                                   # for i in range(len(data[0])):
   515                                                   #     for j in range(len(data)):
   516                                                   #         images[i][j] = data[j][i].crop_tensor_normed.detach().clone()
   517                                           
   518                                                   # move images to gpu
   519      1250     280989.2    224.8      0.1          images = [im.cuda(non_blocking=True) for im in images]
   520                                                   # teacher and student forward passes + compute dino loss
   521      1250      59932.9     47.9      0.0          with torch.cuda.amp.autocast(fp16_scaler is not None):
   522      1250   19870297.3  15896.2      4.1              teacher_output = teacher(images[:2], args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)  # only the 2 global views pass through the teacher
   523      1250   66467433.7  53173.9     13.8              student_output = student(images, args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)
   524      1250   22507059.3  18005.6      4.7              loss = dino_loss(student_output, teacher_output, corrs, epoch)
   525                                           
   526      1250      51106.6     40.9      0.0          if not math.isfinite(loss.item()):
   527                                                       print("Loss is {}, stopping training".format(loss.item()), force=True)
   528                                                       sys.exit(1)
   529                                           
   530                                                   # student update
   531      1250    1624464.6   1299.6      0.3          optimizer.zero_grad()
   532      1250       7175.0      5.7      0.0          param_norms = None
   533      1250        524.1      0.4      0.0          if fp16_scaler is None:
   534                                                       loss.backward()
   535                                                       if args.clip_grad:
   536                                                           param_norms = utils.clip_gradients(student, args.clip_grad)
   537                                                       utils.cancel_gradients_last_layer(epoch, student,
   538                                                                                         args.freeze_last_layer)
   539                                                       optimizer.step()
   540                                                   else:
   541      1250   86910114.5  69528.1     18.1              fp16_scaler.scale(loss).backward()
   542      1250       4369.7      3.5      0.0              if args.clip_grad:
   543      1250    1949660.0   1559.7      0.4                  fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
   544      1250   53604223.9  42883.4     11.2                  param_norms = utils.clip_gradients(student, args.clip_grad)
   545      1250       2839.8      2.3      0.0              utils.cancel_gradients_last_layer(epoch, student,
   546      1250       1082.6      0.9      0.0                                                args.freeze_last_layer)
   547      1250   18421262.6  14737.0      3.8              fp16_scaler.step(optimizer)
   548      1250      56017.0     44.8      0.0              fp16_scaler.update()
   549                                           
   550                                                   # EMA update for the teacher
   551      1250      11778.5      9.4      0.0          with torch.no_grad():
   552      1250       3557.6      2.8      0.0              m = momentum_schedule[it]  # momentum parameter
   553    197500    2862713.6     14.5      0.6              for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):
   554    197500    6384945.2     32.3      1.3                  param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
   555                                           
   556                                                   # logging
   557      1250      82044.9     65.6      0.0          torch.cuda.synchronize()
   558      1250      53796.4     43.0      0.0          metric_logger.update(loss=loss.item())
   559      1250       7656.2      6.1      0.0          metric_logger.update(lr=optimizer.param_groups[0]["lr"])
   560      1250       4586.3      3.7      0.0          metric_logger.update(wd=optimizer.param_groups[0]["weight_decay"])
   561                                               # gather the stats from all processes
   562         1       1299.4   1299.4      0.0      metric_logger.synchronize_between_processes()
   563         1        455.0    455.0      0.0      print("Averaged stats:", metric_logger)
   564         1         15.5     15.5      0.0      return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


Will run the code on one GPU.
| distributed init (rank 0): env://
git:
  sha: 8016efff07cc004d65c1e9eff314f38792840c97, status: has uncommited changes, branch: QCRI_Cifar10_same_batch_augmentation

arch: vit_tiny
batch_size_per_gpu: 40
clip_grad: 3.0
data_path: /home/alij/Datasets/Cifar10/train
dist_url: env://
drop_path_rate: 0.1
epochs: 102
freeze_last_layer: 1
global_crops_number: 2
global_crops_scale: (0.4, 1.0)
global_scale: 224
gpu: 0
local_crops_number: 8
local_crops_scale: (0.05, 0.4)
local_rank: 0
local_scale: 96
lr: 0.0005
min_lr: 1e-06
momentum_teacher: 0.996
norm_last_layer: True
num_workers: 4
optimizer: adamw
out_dim: 1000
output_dir: /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu
patch_size: 16
rank: 0
saveckp_freq: 20
seed: 0
teacher_temp: 0.04
use_bn_in_head: False
use_fp16: True
warmup_epochs: 10
warmup_teacher_temp: 0.04
warmup_teacher_temp_epochs: 0
weight_decay: 0.04
weight_decay_end: 0.4
world_size: 1
Data loaded: there are 50000 images.
Student and Teacher are built: they are both vit_tiny network.
Loss, optimizer and schedulers ready.
Found checkpoint at /home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth
=> loaded 'student' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'teacher' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
=> loaded 'optimizer' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'fp16_scaler' from checkpoint: '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth'
=> loaded 'dino_loss' from checkpoint '/home/alij/RESULTS/Cifar10/Ours/Network_Checkpoints/ali_batch40_on_cpu/checkpoint.pth' with msg <All keys matched successfully>
Starting DINO training !
Epoch: [101/102]  [   0/1250]  eta: 1:57:45  loss: 2.688894 (2.688894)  lr: 0.000001 (0.000001)  wd: 0.399915 (0.399915)  time: 5.652480  data: 3.123118  max mem: 5312
Epoch: [101/102]  [  10/1250]  eta: 0:17:09  loss: 2.609577 (2.588653)  lr: 0.000001 (0.000001)  wd: 0.399915 (0.399915)  time: 0.830585  data: 0.284116  max mem: 5407
Epoch: [101/102]  [  20/1250]  eta: 0:13:24  loss: 2.500217 (2.615604)  lr: 0.000001 (0.000001)  wd: 0.399916 (0.399916)  time: 0.404517  data: 0.000241  max mem: 5409
Epoch: [101/102]  [  30/1250]  eta: 0:12:00  loss: 2.509371 (2.578490)  lr: 0.000001 (0.000001)  wd: 0.399917 (0.399917)  time: 0.458324  data: 0.011004  max mem: 5414
Epoch: [101/102]  [  40/1250]  eta: 0:11:23  loss: 2.621462 (2.604186)  lr: 0.000001 (0.000001)  wd: 0.399919 (0.399917)  time: 0.471569  data: 0.024681  max mem: 5414
Epoch: [101/102]  [  50/1250]  eta: 0:10:57  loss: 2.617677 (2.585480)  lr: 0.000001 (0.000001)  wd: 0.399920 (0.399918)  time: 0.482959  data: 0.025111  max mem: 5414
Epoch: [101/102]  [  60/1250]  eta: 0:10:40  loss: 2.522900 (2.582007)  lr: 0.000001 (0.000001)  wd: 0.399921 (0.399919)  time: 0.483675  data: 0.042935  max mem: 5430
Epoch: [101/102]  [  70/1250]  eta: 0:10:14  loss: 2.508163 (2.608138)  lr: 0.000001 (0.000001)  wd: 0.399923 (0.399919)  time: 0.451005  data: 0.032448  max mem: 5430
Epoch: [101/102]  [  80/1250]  eta: 0:09:59  loss: 2.576822 (2.614006)  lr: 0.000001 (0.000001)  wd: 0.399924 (0.399920)  time: 0.431512  data: 0.003966  max mem: 5430
Epoch: [101/102]  [  90/1250]  eta: 0:09:49  loss: 2.683613 (2.627290)  lr: 0.000001 (0.000001)  wd: 0.399925 (0.399921)  time: 0.464278  data: 0.019131  max mem: 5430
Epoch: [101/102]  [ 100/1250]  eta: 0:09:35  loss: 2.697769 (2.630226)  lr: 0.000001 (0.000001)  wd: 0.399926 (0.399921)  time: 0.453171  data: 0.024329  max mem: 5430
Epoch: [101/102]  [ 110/1250]  eta: 0:09:22  loss: 2.518585 (2.610608)  lr: 0.000001 (0.000001)  wd: 0.399928 (0.399922)  time: 0.424712  data: 0.008474  max mem: 5430
Epoch: [101/102]  [ 120/1250]  eta: 0:09:14  loss: 2.467050 (2.605530)  lr: 0.000001 (0.000001)  wd: 0.399929 (0.399923)  time: 0.441026  data: 0.016191  max mem: 5430
Epoch: [101/102]  [ 130/1250]  eta: 0:09:08  loss: 2.445096 (2.590907)  lr: 0.000001 (0.000001)  wd: 0.399930 (0.399923)  time: 0.471748  data: 0.041442  max mem: 5430
Epoch: [101/102]  [ 140/1250]  eta: 0:08:57  loss: 2.445096 (2.581126)  lr: 0.000001 (0.000001)  wd: 0.399931 (0.399924)  time: 0.447830  data: 0.034310  max mem: 5430
Epoch: [101/102]  [ 150/1250]  eta: 0:08:54  loss: 2.466623 (2.564594)  lr: 0.000001 (0.000001)  wd: 0.399933 (0.399924)  time: 0.455760  data: 0.041771  max mem: 5430
Epoch: [101/102]  [ 160/1250]  eta: 0:08:47  loss: 2.493497 (2.579827)  lr: 0.000001 (0.000001)  wd: 0.399934 (0.399925)  time: 0.479152  data: 0.033037  max mem: 5430
Epoch: [101/102]  [ 170/1250]  eta: 0:08:39  loss: 2.608800 (2.573525)  lr: 0.000001 (0.000001)  wd: 0.399935 (0.399926)  time: 0.450531  data: 0.000339  max mem: 5430
Epoch: [101/102]  [ 180/1250]  eta: 0:08:34  loss: 2.591228 (2.574021)  lr: 0.000001 (0.000001)  wd: 0.399936 (0.399926)  time: 0.452413  data: 0.000322  max mem: 5430
Epoch: [101/102]  [ 190/1250]  eta: 0:08:25  loss: 2.551769 (2.572232)  lr: 0.000001 (0.000001)  wd: 0.399937 (0.399927)  time: 0.441386  data: 0.000392  max mem: 5430
Epoch: [101/102]  [ 200/1250]  eta: 0:08:18  loss: 2.507454 (2.575250)  lr: 0.000001 (0.000001)  wd: 0.399939 (0.399928)  time: 0.426676  data: 0.000373  max mem: 5430
Epoch: [101/102]  [ 210/1250]  eta: 0:08:13  loss: 2.611006 (2.583629)  lr: 0.000001 (0.000001)  wd: 0.399940 (0.399928)  time: 0.446686  data: 0.010761  max mem: 5430
Epoch: [101/102]  [ 220/1250]  eta: 0:08:06  loss: 2.766046 (2.595772)  lr: 0.000001 (0.000001)  wd: 0.399941 (0.399929)  time: 0.442017  data: 0.020299  max mem: 5430
Epoch: [101/102]  [ 230/1250]  eta: 0:08:00  loss: 2.556172 (2.598175)  lr: 0.000001 (0.000001)  wd: 0.399942 (0.399929)  time: 0.433047  data: 0.009958  max mem: 5430
Epoch: [101/102]  [ 240/1250]  eta: 0:07:53  loss: 2.522674 (2.602341)  lr: 0.000001 (0.000001)  wd: 0.399943 (0.399930)  time: 0.436997  data: 0.000392  max mem: 5430
Epoch: [101/102]  [ 250/1250]  eta: 0:07:48  loss: 2.637350 (2.607955)  lr: 0.000001 (0.000001)  wd: 0.399944 (0.399931)  time: 0.442477  data: 0.022274  max mem: 5430
Epoch: [101/102]  [ 260/1250]  eta: 0:07:44  loss: 2.643268 (2.608658)  lr: 0.000001 (0.000001)  wd: 0.399945 (0.399931)  time: 0.476061  data: 0.056278  max mem: 5430
Epoch: [101/102]  [ 270/1250]  eta: 0:07:39  loss: 2.581662 (2.608486)  lr: 0.000001 (0.000001)  wd: 0.399946 (0.399932)  time: 0.470220  data: 0.047844  max mem: 5430
Epoch: [101/102]  [ 280/1250]  eta: 0:07:33  loss: 2.407354 (2.601121)  lr: 0.000001 (0.000001)  wd: 0.399948 (0.399932)  time: 0.444399  data: 0.022024  max mem: 5430
Epoch: [101/102]  [ 290/1250]  eta: 0:07:29  loss: 2.420704 (2.596393)  lr: 0.000001 (0.000001)  wd: 0.399949 (0.399933)  time: 0.456406  data: 0.008459  max mem: 5430
Epoch: [101/102]  [ 300/1250]  eta: 0:07:23  loss: 2.490524 (2.598791)  lr: 0.000001 (0.000001)  wd: 0.399950 (0.399933)  time: 0.450689  data: 0.000372  max mem: 5430
Epoch: [101/102]  [ 310/1250]  eta: 0:07:19  loss: 2.478510 (2.597290)  lr: 0.000001 (0.000001)  wd: 0.399951 (0.399934)  time: 0.464916  data: 0.000377  max mem: 5430
Epoch: [101/102]  [ 320/1250]  eta: 0:07:13  loss: 2.574576 (2.604628)  lr: 0.000001 (0.000001)  wd: 0.399952 (0.399935)  time: 0.454393  data: 0.000285  max mem: 5430
Epoch: [101/102]  [ 330/1250]  eta: 0:07:07  loss: 2.652969 (2.605951)  lr: 0.000001 (0.000001)  wd: 0.399953 (0.399935)  time: 0.422538  data: 0.000263  max mem: 5430
Epoch: [101/102]  [ 340/1250]  eta: 0:07:02  loss: 2.633083 (2.608837)  lr: 0.000001 (0.000001)  wd: 0.399954 (0.399936)  time: 0.437232  data: 0.000477  max mem: 5430
Epoch: [101/102]  [ 350/1250]  eta: 0:06:56  loss: 2.562865 (2.608572)  lr: 0.000001 (0.000001)  wd: 0.399955 (0.399936)  time: 0.431221  data: 0.000484  max mem: 5430
Epoch: [101/102]  [ 360/1250]  eta: 0:06:52  loss: 2.551040 (2.606424)  lr: 0.000001 (0.000001)  wd: 0.399956 (0.399937)  time: 0.443564  data: 0.015143  max mem: 5430
Epoch: [101/102]  [ 370/1250]  eta: 0:06:48  loss: 2.408619 (2.598892)  lr: 0.000001 (0.000001)  wd: 0.399957 (0.399937)  time: 0.479228  data: 0.028858  max mem: 5430
Epoch: [101/102]  [ 380/1250]  eta: 0:06:43  loss: 2.425802 (2.598703)  lr: 0.000001 (0.000001)  wd: 0.399958 (0.399938)  time: 0.481974  data: 0.043663  max mem: 5430
Epoch: [101/102]  [ 390/1250]  eta: 0:06:39  loss: 2.592498 (2.597619)  lr: 0.000001 (0.000001)  wd: 0.399959 (0.399938)  time: 0.471047  data: 0.062033  max mem: 5431
Epoch: [101/102]  [ 400/1250]  eta: 0:06:33  loss: 2.512014 (2.594282)  lr: 0.000001 (0.000001)  wd: 0.399960 (0.399939)  time: 0.438372  data: 0.039581  max mem: 5431
Epoch: [101/102]  [ 410/1250]  eta: 0:06:28  loss: 2.472489 (2.595787)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399940)  time: 0.435980  data: 0.041649  max mem: 5431
Epoch: [101/102]  [ 420/1250]  eta: 0:06:23  loss: 2.505118 (2.596066)  lr: 0.000001 (0.000001)  wd: 0.399961 (0.399940)  time: 0.447996  data: 0.034427  max mem: 5431
Epoch: [101/102]  [ 430/1250]  eta: 0:06:18  loss: 2.562624 (2.598434)  lr: 0.000001 (0.000001)  wd: 0.399962 (0.399941)  time: 0.429090  data: 0.000356  max mem: 5431
Epoch: [101/102]  [ 440/1250]  eta: 0:06:13  loss: 2.609496 (2.599348)  lr: 0.000001 (0.000001)  wd: 0.399963 (0.399941)  time: 0.426793  data: 0.004705  max mem: 5431
Epoch: [101/102]  [ 450/1250]  eta: 0:06:07  loss: 2.552015 (2.597172)  lr: 0.000001 (0.000001)  wd: 0.399964 (0.399942)  time: 0.425737  data: 0.004650  max mem: 5431
Epoch: [101/102]  [ 460/1250]  eta: 0:06:03  loss: 2.544693 (2.597172)  lr: 0.000001 (0.000001)  wd: 0.399965 (0.399942)  time: 0.449542  data: 0.014695  max mem: 5431
Epoch: [101/102]  [ 470/1250]  eta: 0:05:58  loss: 2.579342 (2.599609)  lr: 0.000001 (0.000001)  wd: 0.399966 (0.399943)  time: 0.456601  data: 0.019434  max mem: 5431
Epoch: [101/102]  [ 480/1250]  eta: 0:05:54  loss: 2.556090 (2.598165)  lr: 0.000001 (0.000001)  wd: 0.399967 (0.399943)  time: 0.463407  data: 0.042148  max mem: 5431
Epoch: [101/102]  [ 490/1250]  eta: 0:05:50  loss: 2.484667 (2.596693)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399944)  time: 0.488331  data: 0.081338  max mem: 5431
Epoch: [101/102]  [ 500/1250]  eta: 0:05:44  loss: 2.490458 (2.596311)  lr: 0.000001 (0.000001)  wd: 0.399968 (0.399944)  time: 0.445708  data: 0.044644  max mem: 5431
Epoch: [101/102]  [ 510/1250]  eta: 0:05:39  loss: 2.493929 (2.594741)  lr: 0.000001 (0.000001)  wd: 0.399969 (0.399945)  time: 0.418994  data: 0.005162  max mem: 5431
Epoch: [101/102]  [ 520/1250]  eta: 0:05:35  loss: 2.493929 (2.594735)  lr: 0.000001 (0.000001)  wd: 0.399970 (0.399945)  time: 0.446701  data: 0.051779  max mem: 5431
Epoch: [101/102]  [ 530/1250]  eta: 0:05:30  loss: 2.633148 (2.595546)  lr: 0.000001 (0.000001)  wd: 0.399971 (0.399946)  time: 0.474049  data: 0.065984  max mem: 5431
Epoch: [101/102]  [ 540/1250]  eta: 0:05:26  loss: 2.614360 (2.595658)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399946)  time: 0.464006  data: 0.038223  max mem: 5431
Epoch: [101/102]  [ 550/1250]  eta: 0:05:21  loss: 2.675788 (2.600373)  lr: 0.000001 (0.000001)  wd: 0.399972 (0.399947)  time: 0.459920  data: 0.046045  max mem: 5431
Epoch: [101/102]  [ 560/1250]  eta: 0:05:17  loss: 2.544097 (2.596535)  lr: 0.000001 (0.000001)  wd: 0.399973 (0.399947)  time: 0.466661  data: 0.048532  max mem: 5431
Epoch: [101/102]  [ 570/1250]  eta: 0:05:12  loss: 2.350699 (2.594176)  lr: 0.000001 (0.000001)  wd: 0.399974 (0.399948)  time: 0.440801  data: 0.041092  max mem: 5431
Epoch: [101/102]  [ 580/1250]  eta: 0:05:07  loss: 2.348108 (2.594036)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399948)  time: 0.451677  data: 0.069488  max mem: 5431
Epoch: [101/102]  [ 590/1250]  eta: 0:05:03  loss: 2.485685 (2.596601)  lr: 0.000001 (0.000001)  wd: 0.399975 (0.399949)  time: 0.481160  data: 0.086411  max mem: 5431
Epoch: [101/102]  [ 600/1250]  eta: 0:04:58  loss: 2.485685 (2.594546)  lr: 0.000001 (0.000001)  wd: 0.399976 (0.399949)  time: 0.470929  data: 0.067459  max mem: 5431
Epoch: [101/102]  [ 610/1250]  eta: 0:04:53  loss: 2.525235 (2.597645)  lr: 0.000001 (0.000001)  wd: 0.399977 (0.399950)  time: 0.432879  data: 0.046580  max mem: 5431
Epoch: [101/102]  [ 620/1250]  eta: 0:04:49  loss: 2.738071 (2.602473)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399950)  time: 0.454910  data: 0.057075  max mem: 5431
Epoch: [101/102]  [ 630/1250]  eta: 0:04:44  loss: 2.673080 (2.603669)  lr: 0.000001 (0.000001)  wd: 0.399978 (0.399950)  time: 0.477388  data: 0.079321  max mem: 5431
Epoch: [101/102]  [ 640/1250]  eta: 0:04:40  loss: 2.556138 (2.606463)  lr: 0.000001 (0.000001)  wd: 0.399979 (0.399951)  time: 0.461560  data: 0.046521  max mem: 5431
Epoch: [101/102]  [ 650/1250]  eta: 0:04:35  loss: 2.556138 (2.606502)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399951)  time: 0.444611  data: 0.021978  max mem: 5431
Epoch: [101/102]  [ 660/1250]  eta: 0:04:30  loss: 2.505614 (2.607320)  lr: 0.000001 (0.000001)  wd: 0.399980 (0.399952)  time: 0.426687  data: 0.022227  max mem: 5431
Epoch: [101/102]  [ 670/1250]  eta: 0:04:25  loss: 2.507062 (2.605211)  lr: 0.000001 (0.000001)  wd: 0.399981 (0.399952)  time: 0.439232  data: 0.030069  max mem: 5431
Epoch: [101/102]  [ 680/1250]  eta: 0:04:21  loss: 2.485236 (2.603747)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399953)  time: 0.441613  data: 0.040864  max mem: 5431
Epoch: [101/102]  [ 690/1250]  eta: 0:04:17  loss: 2.485236 (2.601926)  lr: 0.000001 (0.000001)  wd: 0.399982 (0.399953)  time: 0.491322  data: 0.099028  max mem: 5431
Epoch: [101/102]  [ 700/1250]  eta: 0:04:12  loss: 2.373750 (2.599913)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399954)  time: 0.485311  data: 0.102242  max mem: 5431
Epoch: [101/102]  [ 710/1250]  eta: 0:04:08  loss: 2.576634 (2.601425)  lr: 0.000001 (0.000001)  wd: 0.399983 (0.399954)  time: 0.455360  data: 0.047017  max mem: 5431
Epoch: [101/102]  [ 720/1250]  eta: 0:04:03  loss: 2.585357 (2.601885)  lr: 0.000001 (0.000001)  wd: 0.399984 (0.399954)  time: 0.466189  data: 0.052783  max mem: 5431
Epoch: [101/102]  [ 730/1250]  eta: 0:03:58  loss: 2.572561 (2.602932)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399955)  time: 0.441405  data: 0.057875  max mem: 5431
Epoch: [101/102]  [ 740/1250]  eta: 0:03:54  loss: 2.726395 (2.606820)  lr: 0.000001 (0.000001)  wd: 0.399985 (0.399955)  time: 0.459031  data: 0.057304  max mem: 5431
Epoch: [101/102]  [ 750/1250]  eta: 0:03:49  loss: 2.637691 (2.605959)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399956)  time: 0.479072  data: 0.028636  max mem: 5431
Epoch: [101/102]  [ 760/1250]  eta: 0:03:44  loss: 2.593882 (2.609898)  lr: 0.000001 (0.000001)  wd: 0.399986 (0.399956)  time: 0.450192  data: 0.000283  max mem: 5431
Epoch: [101/102]  [ 770/1250]  eta: 0:03:39  loss: 2.562516 (2.610398)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399956)  time: 0.417833  data: 0.000291  max mem: 5431
Epoch: [101/102]  [ 780/1250]  eta: 0:03:35  loss: 2.435387 (2.610175)  lr: 0.000001 (0.000001)  wd: 0.399987 (0.399957)  time: 0.432149  data: 0.020128  max mem: 5431
Epoch: [101/102]  [ 790/1250]  eta: 0:03:30  loss: 2.415414 (2.607508)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399957)  time: 0.442022  data: 0.038851  max mem: 5431
Epoch: [101/102]  [ 800/1250]  eta: 0:03:26  loss: 2.406832 (2.605195)  lr: 0.000001 (0.000001)  wd: 0.399988 (0.399958)  time: 0.463276  data: 0.066936  max mem: 5431
Epoch: [101/102]  [ 810/1250]  eta: 0:03:21  loss: 2.406832 (2.604853)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399958)  time: 0.485417  data: 0.084232  max mem: 5431
Epoch: [101/102]  [ 820/1250]  eta: 0:03:16  loss: 2.493451 (2.604032)  lr: 0.000001 (0.000001)  wd: 0.399989 (0.399958)  time: 0.451900  data: 0.046607  max mem: 5431
Epoch: [101/102]  [ 830/1250]  eta: 0:03:12  loss: 2.619802 (2.606413)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399959)  time: 0.426709  data: 0.010581  max mem: 5431
Epoch: [101/102]  [ 840/1250]  eta: 0:03:07  loss: 2.559388 (2.605482)  lr: 0.000001 (0.000001)  wd: 0.399990 (0.399959)  time: 0.423762  data: 0.000693  max mem: 5431
Epoch: [101/102]  [ 850/1250]  eta: 0:03:02  loss: 2.550537 (2.605557)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399960)  time: 0.404970  data: 0.005566  max mem: 5431
Epoch: [101/102]  [ 860/1250]  eta: 0:02:58  loss: 2.526284 (2.604552)  lr: 0.000001 (0.000001)  wd: 0.399991 (0.399960)  time: 0.443884  data: 0.044198  max mem: 5431
Epoch: [101/102]  [ 870/1250]  eta: 0:02:53  loss: 2.523922 (2.604142)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399960)  time: 0.479909  data: 0.071819  max mem: 5431
Epoch: [101/102]  [ 880/1250]  eta: 0:02:49  loss: 2.668620 (2.606334)  lr: 0.000001 (0.000001)  wd: 0.399992 (0.399961)  time: 0.475119  data: 0.063696  max mem: 5431
Epoch: [101/102]  [ 890/1250]  eta: 0:02:44  loss: 2.589304 (2.605513)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399961)  time: 0.478330  data: 0.031502  max mem: 5431
Epoch: [101/102]  [ 900/1250]  eta: 0:02:40  loss: 2.562381 (2.605573)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399961)  time: 0.450684  data: 0.000600  max mem: 5431
Epoch: [101/102]  [ 910/1250]  eta: 0:02:35  loss: 2.515937 (2.605881)  lr: 0.000001 (0.000001)  wd: 0.399993 (0.399962)  time: 0.428243  data: 0.000274  max mem: 5431
Epoch: [101/102]  [ 920/1250]  eta: 0:02:30  loss: 2.551924 (2.607448)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399962)  time: 0.412980  data: 0.000251  max mem: 5431
Epoch: [101/102]  [ 930/1250]  eta: 0:02:25  loss: 2.655603 (2.607843)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399962)  time: 0.399963  data: 0.000253  max mem: 5431
Epoch: [101/102]  [ 940/1250]  eta: 0:02:21  loss: 2.483729 (2.605512)  lr: 0.000001 (0.000001)  wd: 0.399994 (0.399963)  time: 0.440788  data: 0.029571  max mem: 5431
Epoch: [101/102]  [ 950/1250]  eta: 0:02:16  loss: 2.403658 (2.604466)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399963)  time: 0.434715  data: 0.029579  max mem: 5431
Epoch: [101/102]  [ 960/1250]  eta: 0:02:12  loss: 2.552093 (2.606364)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399963)  time: 0.429081  data: 0.003331  max mem: 5431
Epoch: [101/102]  [ 970/1250]  eta: 0:02:07  loss: 2.674419 (2.606706)  lr: 0.000001 (0.000001)  wd: 0.399995 (0.399964)  time: 0.462113  data: 0.011571  max mem: 5431
Epoch: [101/102]  [ 980/1250]  eta: 0:02:02  loss: 2.496754 (2.607641)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399964)  time: 0.445482  data: 0.008474  max mem: 5431
Epoch: [101/102]  [ 990/1250]  eta: 0:01:58  loss: 2.539131 (2.608312)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399964)  time: 0.432916  data: 0.000244  max mem: 5431
Epoch: [101/102]  [1000/1250]  eta: 0:01:53  loss: 2.491975 (2.606752)  lr: 0.000001 (0.000001)  wd: 0.399996 (0.399965)  time: 0.437389  data: 0.008691  max mem: 5431
Epoch: [101/102]  [1010/1250]  eta: 0:01:49  loss: 2.491975 (2.606321)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399965)  time: 0.455589  data: 0.018958  max mem: 5431
Epoch: [101/102]  [1020/1250]  eta: 0:01:44  loss: 2.632933 (2.607544)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399965)  time: 0.481140  data: 0.060917  max mem: 5431
Epoch: [101/102]  [1030/1250]  eta: 0:01:40  loss: 2.427501 (2.605309)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399966)  time: 0.470317  data: 0.068702  max mem: 5431
Epoch: [101/102]  [1040/1250]  eta: 0:01:35  loss: 2.427501 (2.605812)  lr: 0.000001 (0.000001)  wd: 0.399997 (0.399966)  time: 0.454502  data: 0.025003  max mem: 5431
Epoch: [101/102]  [1050/1250]  eta: 0:01:30  loss: 2.578382 (2.607328)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399966)  time: 0.441631  data: 0.006936  max mem: 5431
Epoch: [101/102]  [1060/1250]  eta: 0:01:26  loss: 2.586229 (2.608612)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.413633  data: 0.012148  max mem: 5431
Epoch: [101/102]  [1070/1250]  eta: 0:01:21  loss: 2.595473 (2.611450)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.447905  data: 0.025421  max mem: 5431
Epoch: [101/102]  [1080/1250]  eta: 0:01:17  loss: 2.540111 (2.612662)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.435611  data: 0.013527  max mem: 5431
Epoch: [101/102]  [1090/1250]  eta: 0:01:12  loss: 2.513858 (2.612348)  lr: 0.000001 (0.000001)  wd: 0.399998 (0.399967)  time: 0.415945  data: 0.008545  max mem: 5431
Epoch: [101/102]  [1100/1250]  eta: 0:01:08  loss: 2.424629 (2.612346)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399968)  time: 0.417538  data: 0.008520  max mem: 5431
Epoch: [101/102]  [1110/1250]  eta: 0:01:03  loss: 2.427606 (2.611840)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399968)  time: 0.424168  data: 0.014479  max mem: 5431
Epoch: [101/102]  [1120/1250]  eta: 0:00:58  loss: 2.583341 (2.611342)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399968)  time: 0.462285  data: 0.041310  max mem: 5431
Epoch: [101/102]  [1130/1250]  eta: 0:00:54  loss: 2.497658 (2.610740)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.436556  data: 0.027049  max mem: 5431
Epoch: [101/102]  [1140/1250]  eta: 0:00:49  loss: 2.461051 (2.610151)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.417098  data: 0.021279  max mem: 5431
Epoch: [101/102]  [1150/1250]  eta: 0:00:45  loss: 2.528382 (2.611278)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.422833  data: 0.042624  max mem: 5431
Epoch: [101/102]  [1160/1250]  eta: 0:00:40  loss: 2.477013 (2.611088)  lr: 0.000001 (0.000001)  wd: 0.399999 (0.399969)  time: 0.441039  data: 0.037661  max mem: 5431
Epoch: [101/102]  [1170/1250]  eta: 0:00:36  loss: 2.567778 (2.612122)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.473731  data: 0.063230  max mem: 5431
Epoch: [101/102]  [1180/1250]  eta: 0:00:31  loss: 2.677926 (2.612635)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.477112  data: 0.083373  max mem: 5431
Epoch: [101/102]  [1190/1250]  eta: 0:00:27  loss: 2.640799 (2.613369)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.458364  data: 0.071652  max mem: 5431
Epoch: [101/102]  [1200/1250]  eta: 0:00:22  loss: 2.711061 (2.614630)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399970)  time: 0.459982  data: 0.068710  max mem: 5431
Epoch: [101/102]  [1210/1250]  eta: 0:00:18  loss: 2.645428 (2.614444)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.456349  data: 0.046381  max mem: 5431
Epoch: [101/102]  [1220/1250]  eta: 0:00:13  loss: 2.579251 (2.614050)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.432059  data: 0.018476  max mem: 5431
Epoch: [101/102]  [1230/1250]  eta: 0:00:09  loss: 2.498531 (2.614080)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.438711  data: 0.025807  max mem: 5431
Epoch: [101/102]  [1240/1250]  eta: 0:00:04  loss: 2.436121 (2.613059)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399971)  time: 0.458549  data: 0.037817  max mem: 5431
Epoch: [101/102]  [1249/1250]  eta: 0:00:00  loss: 2.436121 (2.612796)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)  time: 0.428268  data: 0.017635  max mem: 5431
Epoch: [101/102] Total time: 0:09:26 (0.452827 s / it)
Averaged stats: loss: 2.436121 (2.612796)  lr: 0.000001 (0.000001)  wd: 0.400000 (0.399972)
Training time 0:09:26
Wrote profile results to main_dino.py.lprof
Timer unit: 1e-06 s

Total time: 0 s
File: main_dino.py
Function: collate_function at line 124

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   124                                           @profile
   125                                           def collate_function(batch, additional_arg):
   126                                               process_seed = random.randint(0, 1000000)
   127                                           
   128                                               # Separate the samples and targets
   129                                               samples, targets = zip(*batch)
   130                                           
   131                                               # Apply augmentations to each sample within the batch
   132                                               augmented_samples = []
   133                                               for sample in samples:
   134                                                   augmented_sample = DataAugmentationDINO(additional_arg, sample, process_seed)
   135                                                   augmented_samples.append(augmented_sample)
   136                                           
   137                                               # show_images(augmented_samples, additional_arg.batch_size_per_gpu)
   138                                               
   139                                               corrs = [[None for _ in range(additional_arg.global_crops_number + additional_arg.local_crops_number)] for _ in range(additional_arg.global_crops_number)]
   140                                           
   141                                               # Calculate patch correspondences for the last image in the batch
   142                                               # which is also equal to other images in the batch:
   143                                           
   144                                               for iq in range(additional_arg.global_crops_number):
   145                                                   for v in range(additional_arg.global_crops_number + additional_arg.local_crops_number):
   146                                                       if v == iq:
   147                                                           # we skip cases where student and teacher operate on the same view
   148                                                           continue
   149                                           
   150                                                       corrs[iq][v] = correspondences(augmented_sample[iq], augmented_sample[v])
   151                                               
   152                                               # Return the augmented samples, correspondences, and targets as a batch
   153                                               return augmented_samples, corrs, targets

Total time: 565.058 s
File: main_dino.py
Function: train_one_epoch at line 439

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   439                                           @profile
   440                                           def train_one_epoch(student, teacher, teacher_without_ddp, dino_loss, data_loader,
   441                                                               optimizer, lr_schedule, wd_schedule, momentum_schedule,epoch,
   442                                                               fp16_scaler, args):
   443         1         69.6     69.6      0.0      metric_logger = utils.MetricLogger(delimiter="  ")
   444         1         29.1     29.1      0.0      header = 'Epoch: [{}/{}]'.format(epoch, args.epochs)
   445      1250   83878758.9  67103.0     14.8      for it, (data, corrs, _) in enumerate(metric_logger.log_every(data_loader, 10, header)):
   446                                                   # show_batch_images(data, args.batch_size_per_gpu)
   447                                           
   448                                                   # update weight decay and learning rate according to their schedule
   449      1250      25733.2     20.6      0.0          it = len(data_loader) * epoch + it  # global training iteration
   450      2500       5108.7      2.0      0.0          for i, param_group in enumerate(optimizer.param_groups):
   451      2500       7282.8      2.9      0.0              param_group["lr"] = lr_schedule[it]
   452      1250        674.3      0.5      0.0              if i == 0:  # only the first group is regularized
   453      1250       1502.5      1.2      0.0                  param_group["weight_decay"] = wd_schedule[it]
   454                                           
   455                                                   # Decompose data:
   456      1250     108017.6     86.4      0.0          images =[
   457      1250     135942.3    108.8      0.0              torch.empty((len(data),3,args.global_scale,args.global_scale)),torch.empty((len(data),3,args.global_scale,args.global_scale)),
   458      1250      28635.2     22.9      0.0              torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   459      1250      21726.5     17.4      0.0              torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   460      1250      43120.4     34.5      0.0              torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   461      1250      19232.7     15.4      0.0              torch.empty((len(data),3,args.local_scale,args.local_scale)),torch.empty((len(data),3,args.local_scale,args.local_scale)),
   462                                                   ]
   463     12500       7980.0      0.6      0.0          for i in range(len(data[0])):
   464    500000     190693.4      0.4      0.0              for j in range(len(data)):
   465    500000   71269910.2    142.5     12.6                  images[i][j] = data[j][i].crop_tensor_normed.detach().clone()
   466                                           
   467                                                   # images = [d[0] for d in data]
   468                                                   # images_coordinates = [d[1][0] for d in data]
   469                                                   # images_flips = [d[1][1][0] for d in data]
   470                                           
   471                                                   # move images to gpu
   472      1250   19604626.3  15683.7      3.5          images = [im.cuda(non_blocking=True) for im in images]
   473                                                   # teacher and student forward passes + compute dino loss
   474      1250      73720.5     59.0      0.0          with torch.cuda.amp.autocast(fp16_scaler is not None):
   475      1250   26605973.6  21284.8      4.7              teacher_output = teacher(images[:2], args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)  # only the 2 global views pass through the teacher
   476      1250   80997084.6  64797.7     14.3              student_output = student(images, args.batch_size_per_gpu, args.local_crops_number, args.patch_size, args.global_scale, args.local_scale)
   477      1250   28777457.7  23022.0      5.1              loss = dino_loss(student_output, teacher_output, corrs, epoch)
   478                                           
   479      1250      98004.3     78.4      0.0          if not math.isfinite(loss.item()):
   480                                                       print("Loss is {}, stopping training".format(loss.item()), force=True)
   481                                                       sys.exit(1)
   482                                           
   483                                                   # student update
   484      1250    3815382.7   3052.3      0.7          optimizer.zero_grad()
   485      1250      10001.4      8.0      0.0          param_norms = None
   486      1250        807.7      0.6      0.0          if fp16_scaler is None:
   487                                                       loss.backward()
   488                                                       if args.clip_grad:
   489                                                           param_norms = utils.clip_gradients(student, args.clip_grad)
   490                                                       utils.cancel_gradients_last_layer(epoch, student,
   491                                                                                         args.freeze_last_layer)
   492                                                       optimizer.step()
   493                                                   else:
   494      1250  101103162.4  80882.5     17.9              fp16_scaler.scale(loss).backward()
   495      1250       4798.5      3.8      0.0              if args.clip_grad:
   496      1250    2442628.0   1954.1      0.4                  fp16_scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place
   497      1250   78478673.7  62782.9     13.9                  param_norms = utils.clip_gradients(student, args.clip_grad)
   498      1250       4321.3      3.5      0.0              utils.cancel_gradients_last_layer(epoch, student,
   499      1250       1709.6      1.4      0.0                                                args.freeze_last_layer)
   500      1250   44805482.2  35844.4      7.9              fp16_scaler.step(optimizer)
   501      1250     118010.4     94.4      0.0              fp16_scaler.update()
   502                                           
   503                                                   # EMA update for the teacher
   504      1250      21370.4     17.1      0.0          with torch.no_grad():
   505      1250       5442.9      4.4      0.0              m = momentum_schedule[it]  # momentum parameter
   506    197500    4116202.4     20.8      0.7              for param_q, param_k in zip(student.module.parameters(), teacher_without_ddp.parameters()):
   507    197500   17970088.2     91.0      3.2                  param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
   508                                           
   509                                                   # logging
   510      1250     123862.3     99.1      0.0          torch.cuda.synchronize()
   511      1250     115290.9     92.2      0.0          metric_logger.update(loss=loss.item())
   512      1250      11105.2      8.9      0.0          metric_logger.update(lr=optimizer.param_groups[0]["lr"])
   513      1250       6449.0      5.2      0.0          metric_logger.update(wd=optimizer.param_groups[0]["weight_decay"])
   514                                               # gather the stats from all processes
   515         1       1288.9   1288.9      0.0      metric_logger.synchronize_between_processes()
   516         1        451.6    451.6      0.0      print("Averaged stats:", metric_logger)
   517         1         10.8     10.8      0.0      return {k: meter.global_avg for k, meter in metric_logger.meters.items()}

